{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOhR9MzjhHZsvgpcPM6rYqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshitgosain/Tensorflow-ZTM/blob/main/TF_ZTM_07_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Fundamentals in TF"
      ],
      "metadata": {
        "id": "CKP8rK-46aDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence-to-sequence\n",
        "* One to one- Image Captioning\n",
        "* Many to one- Sentiment analysis\n",
        "* Many to one- time series forecasting\n",
        "* many- to-many- Machine translation\n"
      ],
      "metadata": {
        "id": "UEIfyi-r8kcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfEm1Kes8lgu",
        "outputId": "1bc70d9c-5cbc-4b8b-934c-1f2120d8cf6b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3FIIV0NRezo",
        "outputId": "98153cd2-2a77-4366-b0f8-642a94ce7a6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-16 02:13:34--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-16 02:13:35 (54.8 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
      ],
      "metadata": {
        "id": "FZdua_xCR3nj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu1aEiChSBoa",
        "outputId": "6ec6fc30-5a06-4f8a-b10e-1707b8d38719"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-16 02:13:40--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.218.128, 173.194.217.128, 173.194.215.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.218.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp_getting_started.zip’\n",
            "\n",
            "\rnlp_getting_started   0%[                    ]       0  --.-KB/s               \rnlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-06-16 02:13:40 (88.5 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unzip_data(\"nlp_getting_started.zip\")"
      ],
      "metadata": {
        "id": "ZwhaaRJFSXGF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the dataset\n",
        "\n",
        "To visualize the sample data, we need to read the in. On way is to use python.\n",
        "\n",
        "Another way to do this is use pandas\n",
        "\n"
      ],
      "metadata": {
        "id": "FnODZX-2SZtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df=pd.read_csv(\"train.csv\")\n",
        "test_df=pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "OA7Fu5nwKZJE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "yECy1rYFLgUa",
        "outputId": "923453ee-5594-472a-e962-651bb4daf1d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-465b7f5a-de20-4473-9a2c-8133628bed71\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-465b7f5a-de20-4473-9a2c-8133628bed71')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-465b7f5a-de20-4473-9a2c-8133628bed71 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-465b7f5a-de20-4473-9a2c-8133628bed71');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_shuffled=train_df.sample(frac=1, random_state=42)"
      ],
      "metadata": {
        "id": "6mcpKv3ULfok"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_shuffled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "rheeFHeLMEPe",
        "outputId": "98185088-f370-4d16-ece0-8a6b79491f77"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id       keyword                        location  \\\n",
              "2644   3796   destruction                             NaN   \n",
              "2227   3185        deluge                             NaN   \n",
              "5448   7769        police                              UK   \n",
              "132     191    aftershock                             NaN   \n",
              "6845   9810        trauma           Montgomery County, MD   \n",
              "...     ...           ...                             ...   \n",
              "5226   7470  obliteration                         Merica!   \n",
              "5390   7691         panic                             NaN   \n",
              "860    1242         blood                             NaN   \n",
              "7603  10862           NaN                             NaN   \n",
              "7270  10409     whirlwind  Stamford & Cork (& Shropshire)   \n",
              "\n",
              "                                                   text  target  \n",
              "2644  So you have a new weapon that can cause un-ima...       1  \n",
              "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
              "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
              "132   Aftershock back to school kick off was great. ...       0  \n",
              "6845  in response to trauma Children of Addicts deve...       0  \n",
              "...                                                 ...     ...  \n",
              "5226  @Eganator2000 There aren't many Obliteration s...       0  \n",
              "5390  just had a panic attack bc I don't have enough...       0  \n",
              "860   Omron HEM-712C Automatic Blood Pressure Monito...       0  \n",
              "7603  Officials say a quarantine is in place at an A...       1  \n",
              "7270  I moved to England five years ago today. What ...       1  \n",
              "\n",
              "[7613 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29fc03a6-493e-4294-8f42-1ab5ed2906f3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>3796</td>\n",
              "      <td>destruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>So you have a new weapon that can cause un-ima...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>3185</td>\n",
              "      <td>deluge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>7769</td>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>191</td>\n",
              "      <td>aftershock</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Aftershock back to school kick off was great. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>9810</td>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts deve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5226</th>\n",
              "      <td>7470</td>\n",
              "      <td>obliteration</td>\n",
              "      <td>Merica!</td>\n",
              "      <td>@Eganator2000 There aren't many Obliteration s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>7691</td>\n",
              "      <td>panic</td>\n",
              "      <td>NaN</td>\n",
              "      <td>just had a panic attack bc I don't have enough...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>1242</td>\n",
              "      <td>blood</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Omron HEM-712C Automatic Blood Pressure Monito...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7603</th>\n",
              "      <td>10862</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Officials say a quarantine is in place at an A...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7270</th>\n",
              "      <td>10409</td>\n",
              "      <td>whirlwind</td>\n",
              "      <td>Stamford &amp; Cork (&amp; Shropshire)</td>\n",
              "      <td>I moved to England five years ago today. What ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7613 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29fc03a6-493e-4294-8f42-1ab5ed2906f3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-29fc03a6-493e-4294-8f42-1ab5ed2906f3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-29fc03a6-493e-4294-8f42-1ab5ed2906f3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test dataframe looks like:\n",
        "test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EOKnKXMgMLmz",
        "outputId": "f47b2a8e-b398-4696-98bb-631deeab23dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c9c06f1-1b84-4a69-87e3-34c332760351\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c9c06f1-1b84-4a69-87e3-34c332760351')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c9c06f1-1b84-4a69-87e3-34c332760351 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c9c06f1-1b84-4a69-87e3-34c332760351');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVroyArwMSAd",
        "outputId": "f67fc5ff-6e4f-4286-94c5-fef638c8a1dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#How many total number of samples\n",
        "len(train_df), len(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJwjZWDvMbP0",
        "outputId": "449fa239-f2a2-4dd1-87db-c70432b54663"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7613, 3263)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's visualize some random training samples\n",
        "import random\n",
        "random_index = random.randint(0,len(train_df)-5)\n",
        "for row in train_data_shuffled[['text','target']][random_index:random_index+5].itertuples():\n",
        "  _,text, target=row\n",
        "  print(f\"Target: {target}\", \"(real disaster)\" if target >0 else \"(not real disaster)\")\n",
        "  print(f\"text:\\n{text}\\n\")\n",
        "  print(\"---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDoIh77fM8xb",
        "outputId": "5c898af5-904e-4d11-f4ad-7580a40492c6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target: 0 (not real disaster)\n",
            "text:\n",
            "burned 202 calories doing 30 minutes of Walking 4.0 mph very brisk pace #myfitnesspal\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster)\n",
            "text:\n",
            "Couples having less sex... for fear it'll be a let down: Internet movies and books saying how sex 'ought to be' pÛ_ http://t.co/c1xhIzPrAd\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "text:\n",
            "VICTORINOX SWISS ARMY DATE WOMEN'S RUBBER MOP WATCH 241487 http://t.co/yFy3nkkcoH http://t.co/KNEhVvOHVK\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster)\n",
            "text:\n",
            "What. The. Fuck. https://t.co/Nv7rK63Pgc\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "text:\n",
            "#LoMasVisto THOUSANDS OF HIPSTERS FEARED LOST: Giant Sinkhole Devours Brooklyn Intersectio... http://t.co/qwtk1b2fMC #CadenaDeSeguidores\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data into training and validation sets\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3_KYNkVORfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "K-_DDtcAeap3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, val_sentences, train_labels, val_labels=train_test_split(train_data_shuffled['text'].to_numpy(),\n",
        "                                                                          train_data_shuffled['target'].to_numpy(),\n",
        "                                                                          test_size=0.1,\n",
        "                                                                          random_state=42)"
      ],
      "metadata": {
        "id": "rFRtXZeded8h"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the lengths\n",
        "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvV1nQX6fD8S",
        "outputId": "34183fd6-fe4e-4db0-baf2-efa22a06373c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6851, 6851, 762, 762)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting words into numbers\n",
        "\n",
        "First thing while building a model is to convert your text into numbers\n"
      ],
      "metadata": {
        "id": "xQJN3MYYktnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization VS Embedding\n",
        "Tkenization - word level and character level. Direct apping of a token.\n",
        "\n",
        "Embedding - every word gets turned into a vector and we can define size of the vector. Embeddings can learn as our model trains\n",
        "\n"
      ],
      "metadata": {
        "id": "6GOhJ-mcfRFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text vectorization(tokenization)"
      ],
      "metadata": {
        "id": "CZ_FBHbwjhjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "metadata": {
        "id": "otEJGiTzJmNX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the default text vectorization parameters\n",
        "text_vectoizer= TextVectorization(max_tokens=None, #Defines how many words in our vocab\n",
        "                               standardize= \"lower_and_strip_punctuation\",\n",
        "                               split=\"whitespace\",\n",
        "                               ngrams=None,\n",
        "                               output_mode='int',\n",
        "                               output_sequence_length=None) #how long do you want your sequences to be\n",
        "                               #pad_to_max_tokens=True)"
      ],
      "metadata": {
        "id": "1iQnXkkeJvaQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[0].split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7P5w8FmXiRk",
        "outputId": "f7d3e87c-5fdf-4eec-877e-2716c0e32578"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@mogacola', '@zamtriossu', 'i', 'screamed', 'after', 'hitting', 'tweet']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the average number of tokens(words) in the training tweets\n",
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUNdYmJ9WjZY",
        "outputId": "52650f28-381b-4c7e-e191-07a4959b2876"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_length= 10000 #max no of words to have in our vocab\n",
        "max_length=15 #Max length our sequences will be (e.g. how many words from a Tweet does a model see)\n",
        "\n",
        "text_vectorizer=TextVectorization(max_tokens=max_vocab_length,\n",
        "                                     output_mode=\"int\",\n",
        "                                     output_sequence_length=max_length)"
      ],
      "metadata": {
        "id": "LMx1h8VWX4uS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the text vectorizer to the training text\n",
        "text_vectorizer.adapt(train_sentences)\n"
      ],
      "metadata": {
        "id": "yo-nO5JSYXiy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIUXVbetZ-sI",
        "outputId": "374f5d1c-1a78-4436-f896-f42632955ba1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "       'Imagine getting flattened by Kurt Zouma',\n",
              "       '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "       \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "       'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
              "       '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
              "       'destroy the free fandom honestly',\n",
              "       'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
              "       '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
              "       'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a sample sentence and tokenize it\n",
        "sample_sentence=\"There is a floow in my street!\"\n",
        "text_vectorizer([sample_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfFwEaKYaAoe",
        "outputId": "f9a49b3d-03c2-416e-8524-6ef21b9e7e85"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[ 74,   9,   3,   1,   4,  13, 698,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_sentence=random.choice(train_sentences)\n",
        "print(random_sentence)\n",
        "vectorized_random_sentence=text_vectorizer([random_sentence])\n",
        "print(f\"Random Sentence: {random_sentence}\\nVectoized Random Sentence: {vectorized_random_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfpgC8tgaN4z",
        "outputId": "49bd7ec7-5c39-4435-d830-02ac820f682e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".@StacDemon with five burning questions for Chris Mullin and St. JohnÛªs in 2015-16: http://t.co/NmRVTHkvAh #SJUBB\n",
            "Random Sentence: .@StacDemon with five burning questions for Chris Mullin and St. JohnÛªs in 2015-16: http://t.co/NmRVTHkvAh #SJUBB\n",
            "Vectoized Random Sentence: [[8152   14 1525   86 2012   10 5984    1    7  527    1    4    1    1\n",
            "  8462]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_vocab=text_vectorizer.get_vocabulary()\n",
        "top_5_words=words_in_vocab[:5]\n",
        "botton_5_words=words_in_vocab[-5:]"
      ],
      "metadata": {
        "id": "ijV2vvTqbNLm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_vocab,top_5_words,botton_5_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WMshKSybv4a",
        "outputId": "734c7cea-17ce-4608-cf8d-f3c32a483607"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['',\n",
              "  '[UNK]',\n",
              "  'the',\n",
              "  'a',\n",
              "  'in',\n",
              "  'to',\n",
              "  'of',\n",
              "  'and',\n",
              "  'i',\n",
              "  'is',\n",
              "  'for',\n",
              "  'on',\n",
              "  'you',\n",
              "  'my',\n",
              "  'with',\n",
              "  'it',\n",
              "  'that',\n",
              "  'at',\n",
              "  'by',\n",
              "  'this',\n",
              "  'from',\n",
              "  'be',\n",
              "  'are',\n",
              "  'was',\n",
              "  'have',\n",
              "  'like',\n",
              "  'as',\n",
              "  'up',\n",
              "  'so',\n",
              "  'just',\n",
              "  'but',\n",
              "  'me',\n",
              "  'im',\n",
              "  'your',\n",
              "  'not',\n",
              "  'amp',\n",
              "  'out',\n",
              "  'its',\n",
              "  'will',\n",
              "  'an',\n",
              "  'no',\n",
              "  'has',\n",
              "  'fire',\n",
              "  'after',\n",
              "  'all',\n",
              "  'when',\n",
              "  'we',\n",
              "  'if',\n",
              "  'now',\n",
              "  'via',\n",
              "  'new',\n",
              "  'more',\n",
              "  'get',\n",
              "  'or',\n",
              "  'about',\n",
              "  'what',\n",
              "  'he',\n",
              "  'people',\n",
              "  'news',\n",
              "  'been',\n",
              "  'over',\n",
              "  'one',\n",
              "  'how',\n",
              "  'dont',\n",
              "  'they',\n",
              "  'who',\n",
              "  'into',\n",
              "  'were',\n",
              "  'do',\n",
              "  'us',\n",
              "  '2',\n",
              "  'can',\n",
              "  'video',\n",
              "  'emergency',\n",
              "  'there',\n",
              "  'disaster',\n",
              "  'than',\n",
              "  'police',\n",
              "  'would',\n",
              "  'his',\n",
              "  'still',\n",
              "  'her',\n",
              "  'some',\n",
              "  'body',\n",
              "  'storm',\n",
              "  'crash',\n",
              "  'burning',\n",
              "  'suicide',\n",
              "  'back',\n",
              "  'man',\n",
              "  'california',\n",
              "  'why',\n",
              "  'time',\n",
              "  'them',\n",
              "  'had',\n",
              "  'buildings',\n",
              "  'rt',\n",
              "  'first',\n",
              "  'cant',\n",
              "  'see',\n",
              "  'got',\n",
              "  'day',\n",
              "  'off',\n",
              "  'our',\n",
              "  'going',\n",
              "  'nuclear',\n",
              "  'know',\n",
              "  'world',\n",
              "  'bomb',\n",
              "  'fires',\n",
              "  'love',\n",
              "  'killed',\n",
              "  'go',\n",
              "  'attack',\n",
              "  'youtube',\n",
              "  'dead',\n",
              "  'two',\n",
              "  'families',\n",
              "  '3',\n",
              "  'train',\n",
              "  'full',\n",
              "  'being',\n",
              "  'war',\n",
              "  'many',\n",
              "  'today',\n",
              "  'think',\n",
              "  'only',\n",
              "  'car',\n",
              "  'accident',\n",
              "  'life',\n",
              "  'hiroshima',\n",
              "  'their',\n",
              "  'say',\n",
              "  'may',\n",
              "  'down',\n",
              "  'watch',\n",
              "  'good',\n",
              "  'could',\n",
              "  'want',\n",
              "  'last',\n",
              "  'here',\n",
              "  'years',\n",
              "  'u',\n",
              "  'then',\n",
              "  'make',\n",
              "  'did',\n",
              "  'wildfire',\n",
              "  'way',\n",
              "  'help',\n",
              "  'best',\n",
              "  'too',\n",
              "  'even',\n",
              "  'because',\n",
              "  'home',\n",
              "  'death',\n",
              "  'collapse',\n",
              "  'bombing',\n",
              "  'mass',\n",
              "  'him',\n",
              "  'black',\n",
              "  'am',\n",
              "  'those',\n",
              "  'need',\n",
              "  'fatal',\n",
              "  'army',\n",
              "  'another',\n",
              "  'work',\n",
              "  'take',\n",
              "  'should',\n",
              "  'really',\n",
              "  'please',\n",
              "  'mh370',\n",
              "  'youre',\n",
              "  'look',\n",
              "  'lol',\n",
              "  'hot',\n",
              "  'pm',\n",
              "  'legionnaires',\n",
              "  '4',\n",
              "  'right',\n",
              "  '5',\n",
              "  'let',\n",
              "  'city',\n",
              "  'year',\n",
              "  'wreck',\n",
              "  'school',\n",
              "  'northern',\n",
              "  'much',\n",
              "  'forest',\n",
              "  'bomber',\n",
              "  'water',\n",
              "  'she',\n",
              "  'never',\n",
              "  'read',\n",
              "  'latest',\n",
              "  'homes',\n",
              "  'great',\n",
              "  'every',\n",
              "  '1',\n",
              "  'live',\n",
              "  'god',\n",
              "  'fear',\n",
              "  'any',\n",
              "  '\\x89Û',\n",
              "  'under',\n",
              "  'said',\n",
              "  'old',\n",
              "  'floods',\n",
              "  '2015',\n",
              "  'getting',\n",
              "  'atomic',\n",
              "  'while',\n",
              "  'top',\n",
              "  'obama',\n",
              "  'feel',\n",
              "  'thats',\n",
              "  'since',\n",
              "  'near',\n",
              "  'flames',\n",
              "  'ever',\n",
              "  'come',\n",
              "  'where',\n",
              "  'these',\n",
              "  'military',\n",
              "  'japan',\n",
              "  'found',\n",
              "  'content',\n",
              "  'ass',\n",
              "  'without',\n",
              "  'weather',\n",
              "  'most',\n",
              "  'flooding',\n",
              "  'flood',\n",
              "  'damage',\n",
              "  'which',\n",
              "  'shit',\n",
              "  's',\n",
              "  'hope',\n",
              "  'everyone',\n",
              "  'before',\n",
              "  'stop',\n",
              "  'plan',\n",
              "  'malaysia',\n",
              "  'injured',\n",
              "  'hit',\n",
              "  'evacuation',\n",
              "  'during',\n",
              "  'debris',\n",
              "  'cross',\n",
              "  'coming',\n",
              "  'wild',\n",
              "  'well',\n",
              "  'times',\n",
              "  'sinking',\n",
              "  'oil',\n",
              "  'fucking',\n",
              "  'check',\n",
              "  'cause',\n",
              "  'weapons',\n",
              "  'truck',\n",
              "  'food',\n",
              "  'bloody',\n",
              "  'always',\n",
              "  'weapon',\n",
              "  'theres',\n",
              "  'state',\n",
              "  'little',\n",
              "  'injuries',\n",
              "  'free',\n",
              "  'wounded',\n",
              "  'summer',\n",
              "  'smoke',\n",
              "  'severe',\n",
              "  'reddit',\n",
              "  'next',\n",
              "  'movie',\n",
              "  'ive',\n",
              "  'hes',\n",
              "  'fall',\n",
              "  'evacuate',\n",
              "  'confirmed',\n",
              "  'bad',\n",
              "  'again',\n",
              "  'thunderstorm',\n",
              "  'set',\n",
              "  'night',\n",
              "  'natural',\n",
              "  'looks',\n",
              "  'heat',\n",
              "  'face',\n",
              "  'earthquake',\n",
              "  'boy',\n",
              "  'whole',\n",
              "  'until',\n",
              "  'thunder',\n",
              "  'through',\n",
              "  'says',\n",
              "  'panic',\n",
              "  'outbreak',\n",
              "  'made',\n",
              "  'lightning',\n",
              "  'fatalities',\n",
              "  'family',\n",
              "  'explosion',\n",
              "  'end',\n",
              "  'destroy',\n",
              "  'derailment',\n",
              "  'air',\n",
              "  'w',\n",
              "  'terrorist',\n",
              "  'survive',\n",
              "  'screaming',\n",
              "  'saudi',\n",
              "  'refugees',\n",
              "  'rain',\n",
              "  'murder',\n",
              "  'loud',\n",
              "  'liked',\n",
              "  'house',\n",
              "  'gonna',\n",
              "  'failure',\n",
              "  'collided',\n",
              "  'bag',\n",
              "  'attacked',\n",
              "  'ambulance',\n",
              "  '70',\n",
              "  'wind',\n",
              "  'services',\n",
              "  'save',\n",
              "  'report',\n",
              "  'migrants',\n",
              "  'head',\n",
              "  'explode',\n",
              "  'charged',\n",
              "  'change',\n",
              "  'big',\n",
              "  'also',\n",
              "  'wrecked',\n",
              "  'warning',\n",
              "  'update',\n",
              "  'run',\n",
              "  'rescuers',\n",
              "  'released',\n",
              "  'photo',\n",
              "  'massacre',\n",
              "  'injury',\n",
              "  'hurricane',\n",
              "  'high',\n",
              "  'hail',\n",
              "  'fuck',\n",
              "  'does',\n",
              "  'destroyed',\n",
              "  'bus',\n",
              "  'blood',\n",
              "  '40',\n",
              "  '\\x89ÛÒ',\n",
              "  'wreckage',\n",
              "  'violent',\n",
              "  'twister',\n",
              "  'trauma',\n",
              "  'tragedy',\n",
              "  'terrorism',\n",
              "  'survivors',\n",
              "  'survived',\n",
              "  'sinkhole',\n",
              "  'sandstorm',\n",
              "  'road',\n",
              "  'rioting',\n",
              "  'red',\n",
              "  'real',\n",
              "  'put',\n",
              "  'post',\n",
              "  'national',\n",
              "  'missing',\n",
              "  'landslide',\n",
              "  'keep',\n",
              "  'girl',\n",
              "  'drought',\n",
              "  'curfew',\n",
              "  'breaking',\n",
              "  'bags',\n",
              "  'white',\n",
              "  'twitter',\n",
              "  'tonight',\n",
              "  'structural',\n",
              "  'spill',\n",
              "  'service',\n",
              "  'screamed',\n",
              "  'rescued',\n",
              "  'rescue',\n",
              "  'phone',\n",
              "  'ok',\n",
              "  'oh',\n",
              "  'mosque',\n",
              "  'lives',\n",
              "  'horrible',\n",
              "  'harm',\n",
              "  'game',\n",
              "  'dust',\n",
              "  'destruction',\n",
              "  'deluge',\n",
              "  'deaths',\n",
              "  'crashed',\n",
              "  'cliff',\n",
              "  'catastrophe',\n",
              "  'boat',\n",
              "  'away',\n",
              "  'august',\n",
              "  'area',\n",
              "  'apocalypse',\n",
              "  'woman',\n",
              "  'whirlwind',\n",
              "  'traumatised',\n",
              "  'stock',\n",
              "  'saw',\n",
              "  'ruin',\n",
              "  'riot',\n",
              "  'quarantine',\n",
              "  'kills',\n",
              "  'island',\n",
              "  'investigators',\n",
              "  'ill',\n",
              "  'hostages',\n",
              "  'hazard',\n",
              "  'danger',\n",
              "  'call',\n",
              "  '15',\n",
              "  'women',\n",
              "  'windstorm',\n",
              "  'things',\n",
              "  'suspect',\n",
              "  'show',\n",
              "  'reunion',\n",
              "  'quarantined',\n",
              "  'lava',\n",
              "  'heart',\n",
              "  'engulfed',\n",
              "  'detonate',\n",
              "  'crush',\n",
              "  'collapsed',\n",
              "  'came',\n",
              "  'better',\n",
              "  'battle',\n",
              "  'armageddon',\n",
              "  'airplane',\n",
              "  'against',\n",
              "  'affected',\n",
              "  'use',\n",
              "  'trapped',\n",
              "  'thank',\n",
              "  'sunk',\n",
              "  'story',\n",
              "  'send',\n",
              "  'part',\n",
              "  'other',\n",
              "  'must',\n",
              "  'mudslide',\n",
              "  'market',\n",
              "  'iran',\n",
              "  'famine',\n",
              "  'exploded',\n",
              "  'electrocuted',\n",
              "  'ebay',\n",
              "  'displaced',\n",
              "  'derailed',\n",
              "  'derail',\n",
              "  'burned',\n",
              "  'bombed',\n",
              "  'blown',\n",
              "  'baby',\n",
              "  'around',\n",
              "  'zone',\n",
              "  'wave',\n",
              "  'wanna',\n",
              "  'sure',\n",
              "  'someone',\n",
              "  'screams',\n",
              "  'razed',\n",
              "  'power',\n",
              "  'obliterated',\n",
              "  'long',\n",
              "  'land',\n",
              "  'hundreds',\n",
              "  'heard',\n",
              "  'group',\n",
              "  'flattened',\n",
              "  'drown',\n",
              "  'doing',\n",
              "  'care',\n",
              "  'bridge',\n",
              "  'bagging',\n",
              "  '9',\n",
              "  'went',\n",
              "  'used',\n",
              "  'typhoon',\n",
              "  'trouble',\n",
              "  'tornado',\n",
              "  'thought',\n",
              "  'thing',\n",
              "  'river',\n",
              "  'responders',\n",
              "  'past',\n",
              "  'pandemonium',\n",
              "  'officials',\n",
              "  'meltdown',\n",
              "  'lot',\n",
              "  'least',\n",
              "  'inundated',\n",
              "  'id',\n",
              "  'hostage',\n",
              "  'hijacking',\n",
              "  'hazardous',\n",
              "  'goes',\n",
              "  'drowning',\n",
              "  'didnt',\n",
              "  'devastation',\n",
              "  'demolish',\n",
              "  'collide',\n",
              "  'casualties',\n",
              "  'calgary',\n",
              "  'bang',\n",
              "  'anniversary',\n",
              "  'yet',\n",
              "  'wounds',\n",
              "  'volcano',\n",
              "  'tsunami',\n",
              "  'sue',\n",
              "  'st',\n",
              "  'song',\n",
              "  'something',\n",
              "  'shoulder',\n",
              "  'security',\n",
              "  'prebreak',\n",
              "  'possible',\n",
              "  'pkk',\n",
              "  'panicking',\n",
              "  'obliteration',\n",
              "  'obliterate',\n",
              "  'murderer',\n",
              "  'minute',\n",
              "  'light',\n",
              "  'lets',\n",
              "  'kill',\n",
              "  'isis',\n",
              "  'india',\n",
              "  'hijacker',\n",
              "  'hellfire',\n",
              "  'government',\n",
              "  'few',\n",
              "  'evacuated',\n",
              "  'due',\n",
              "  'detonated',\n",
              "  'desolation',\n",
              "  'crushed',\n",
              "  'chemical',\n",
              "  'blew',\n",
              "  'blazing',\n",
              "  'blast',\n",
              "  'annihilated',\n",
              "  'airport',\n",
              "  '6',\n",
              "  'week',\n",
              "  'upheaval',\n",
              "  'trying',\n",
              "  'three',\n",
              "  'thanks',\n",
              "  'sound',\n",
              "  'soon',\n",
              "  'sirens',\n",
              "  'rainstorm',\n",
              "  'plane',\n",
              "  'music',\n",
              "  'making',\n",
              "  'kids',\n",
              "  'issues',\n",
              "  'half',\n",
              "  'guys',\n",
              "  'fedex',\n",
              "  'done',\n",
              "  'died',\n",
              "  'detonation',\n",
              "  'days',\n",
              "  'cyclone',\n",
              "  'county',\n",
              "  'collision',\n",
              "  'caused',\n",
              "  'catastrophic',\n",
              "  'bleeding',\n",
              "  'beautiful',\n",
              "  '8',\n",
              "  'words',\n",
              "  'very',\n",
              "  'traffic',\n",
              "  'south',\n",
              "  'remember',\n",
              "  'policy',\n",
              "  'place',\n",
              "  'nothing',\n",
              "  'north',\n",
              "  'mp',\n",
              "  'longer',\n",
              "  'left',\n",
              "  'israeli',\n",
              "  'hell',\n",
              "  'fun',\n",
              "  'drowned',\n",
              "  'demolished',\n",
              "  'cool',\n",
              "  'both',\n",
              "  'bioterror',\n",
              "  'believe',\n",
              "  'avalanche',\n",
              "  'arson',\n",
              "  'turkey',\n",
              "  'snowstorm',\n",
              "  'site',\n",
              "  'shot',\n",
              "  'shooting',\n",
              "  'pic',\n",
              "  'nowplaying',\n",
              "  'media',\n",
              "  'islam',\n",
              "  'inside',\n",
              "  'hijack',\n",
              "  'helicopter',\n",
              "  'fight',\n",
              "  'fatality',\n",
              "  'fan',\n",
              "  'electrocute',\n",
              "  'doesnt',\n",
              "  'building',\n",
              "  'brown',\n",
              "  'bc',\n",
              "  'actually',\n",
              "  '16yr',\n",
              "  'yes',\n",
              "  'watching',\n",
              "  'wait',\n",
              "  'ur',\n",
              "  'tell',\n",
              "  'swallowed',\n",
              "  'seismic',\n",
              "  'second',\n",
              "  'rubble',\n",
              "  're\\x89Û',\n",
              "  'plans',\n",
              "  'men',\n",
              "  'memories',\n",
              "  'line',\n",
              "  'la',\n",
              "  'horror',\n",
              "  'health',\n",
              "  'having',\n",
              "  'find',\n",
              "  'eyewitness',\n",
              "  'deluged',\n",
              "  'children',\n",
              "  'bush',\n",
              "  'anything',\n",
              "  'already',\n",
              "  'almost',\n",
              "  'aircraft',\n",
              "  'yourself',\n",
              "  'yeah',\n",
              "  'whats',\n",
              "  'tomorrow',\n",
              "  'such',\n",
              "  'start',\n",
              "  'side',\n",
              "  'searching',\n",
              "  'saved',\n",
              "  'reactor',\n",
              "  'probably',\n",
              "  'play',\n",
              "  'person',\n",
              "  'peace',\n",
              "  'outside',\n",
              "  'officer',\n",
              "  'nearby',\n",
              "  'n',\n",
              "  'maybe',\n",
              "  'lost',\n",
              "  'literally',\n",
              "  'hours',\n",
              "  'hear',\n",
              "  'far',\n",
              "  'die',\n",
              "  'demolition',\n",
              "  'data',\n",
              "  'crews',\n",
              "  'conclusively',\n",
              "  'business',\n",
              "  'american',\n",
              "  '20',\n",
              "  '\\x89ÛÓ',\n",
              "  'west',\n",
              "  'waves',\n",
              "  'team',\n",
              "  'street',\n",
              "  'stay',\n",
              "  'soudelor',\n",
              "  'reuters',\n",
              "  'manslaughter',\n",
              "  'leather',\n",
              "  'job',\n",
              "  'history',\n",
              "  'hey',\n",
              "  'feeling',\n",
              "  'eyes',\n",
              "  'everything',\n",
              "  'declares',\n",
              "  'deal',\n",
              "  'casualty',\n",
              "  'bodies',\n",
              "  'amid',\n",
              "  'ablaze',\n",
              "  '7',\n",
              "  '50',\n",
              "  '30',\n",
              "  '12',\n",
              "  'youth',\n",
              "  'wont',\n",
              "  'wake',\n",
              "  'theyre',\n",
              "  'support',\n",
              "  'stretcher',\n",
              "  'same',\n",
              "  'rise',\n",
              "  'picking',\n",
              "  'photos',\n",
              "  'own',\n",
              "  'others',\n",
              "  'order',\n",
              "  'omg',\n",
              "  'okay',\n",
              "  'name',\n",
              "  'myself',\n",
              "  'money',\n",
              "  'makes',\n",
              "  'leave',\n",
              "  'lab',\n",
              "  'gt',\n",
              "  'gets',\n",
              "  'flag',\n",
              "  'desolate',\n",
              "  'crisis',\n",
              "  'center',\n",
              "  'book',\n",
              "  'blight',\n",
              "  'blaze',\n",
              "  'ago',\n",
              "  'abc',\n",
              "  '11yearold',\n",
              "  'womens',\n",
              "  'typhoondevastated',\n",
              "  'tv',\n",
              "  'trench',\n",
              "  'trains',\n",
              "  'texas',\n",
              "  'space',\n",
              "  'siren',\n",
              "  'shes',\n",
              "  'self',\n",
              "  'saipan',\n",
              "  'reason',\n",
              "  'rd',\n",
              "  'pretty',\n",
              "  'pick',\n",
              "  'offensive',\n",
              "  'move',\n",
              "  'meek',\n",
              "  'major',\n",
              "  'm',\n",
              "  'low',\n",
              "  'lord',\n",
              "  'huge',\n",
              "  'hat',\n",
              "  'flash',\n",
              "  'feared',\n",
              "  'fast',\n",
              "  'effect',\n",
              "  'course',\n",
              "  'country',\n",
              "  'control',\n",
              "  'class',\n",
              "  'child',\n",
              "  'chance',\n",
              "  'caught',\n",
              "  'called',\n",
              "  'bioterrorism',\n",
              "  'bestnaijamade',\n",
              "  'become',\n",
              "  'bar',\n",
              "  'banned',\n",
              "  'ball',\n",
              "  'aug',\n",
              "  'annihilation',\n",
              "  'wrong',\n",
              "  'win',\n",
              "  'usa',\n",
              "  'united',\n",
              "  'town',\n",
              "  'totally',\n",
              "  'toddler',\n",
              "  'though',\n",
              "  'temple',\n",
              "  'taken',\n",
              "  'stand',\n",
              "  'spot',\n",
              "  'signs',\n",
              "  'ship',\n",
              "  'pakistan',\n",
              "  'online',\n",
              "  'level',\n",
              "  'ladies',\n",
              "  'jobs',\n",
              "  'isnt',\n",
              "  'happy',\n",
              "  'hailstorm',\n",
              "  'friends',\n",
              "  'disea',\n",
              "  'damn',\n",
              "  'couple',\n",
              "  'case',\n",
              "  'blue',\n",
              "  'bigger',\n",
              "  'america',\n",
              "  'across',\n",
              "  '10',\n",
              "  'yours',\n",
              "  'village',\n",
              "  'try',\n",
              "  'transport',\n",
              "  'talk',\n",
              "  'seen',\n",
              "  'russian',\n",
              "  'radio',\n",
              "  'projected',\n",
              "  'once',\n",
              "  'official',\n",
              "  'needs',\n",
              "  'nearly',\n",
              "  'mount',\n",
              "  'might',\n",
              "  'mayhem',\n",
              "  'instead',\n",
              "  'hollywood',\n",
              "  'haha',\n",
              "  'guy',\n",
              "  'gun',\n",
              "  'green',\n",
              "  'front',\n",
              "  'finally',\n",
              "  'favorite',\n",
              "  'experts',\n",
              "  'entire',\n",
              "  'east',\n",
              "  'daily',\n",
              "  'crazy',\n",
              "  'computers',\n",
              "  'coaches',\n",
              "  'christian',\n",
              "  'china',\n",
              "  'blizzard',\n",
              "  'anyone',\n",
              "  'aint',\n",
              "  'action',\n",
              "  '25',\n",
              "  'virgin',\n",
              "  'vehicle',\n",
              "  'truth',\n",
              "  'trust',\n",
              "  'takes',\n",
              "  't',\n",
              "  'star',\n",
              "  'sorry',\n",
              "  'running',\n",
              "  'refugio',\n",
              "  'reddits',\n",
              "  'poor',\n",
              "  'pain',\n",
              "  'mom',\n",
              "  'miners',\n",
              "  'marks',\n",
              "  'looking',\n",
              "  'knock',\n",
              "  'issued',\n",
              "  'insurance',\n",
              "  'ignition',\n",
              "  'houses',\n",
              "  'heavy',\n",
              "  'hate',\n",
              "  'hard',\n",
              "  'happened',\n",
              "  'global',\n",
              "  'giant',\n",
              "  'gbbo',\n",
              "  'flight',\n",
              "  'eye',\n",
              "  'emmerdale',\n",
              "  'driver',\n",
              "  'devastated',\n",
              "  'd',\n",
              "  'costlier',\n",
              "  'cnn',\n",
              "  'cars',\n",
              "  'camp',\n",
              "  'beach',\n",
              "  'arsonist',\n",
              "  'angry',\n",
              "  'alone',\n",
              "  'added',\n",
              "  '05',\n",
              "  'york',\n",
              "  'wonder',\n",
              "  'uk',\n",
              "  'turn',\n",
              "  'taking',\n",
              "  'subreddits',\n",
              "  'sounds',\n",
              "  'scared',\n",
              "  'russia',\n",
              "  'rly',\n",
              "  'reports',\n",
              "  'ready',\n",
              "  'quiz',\n",
              "  'public',\n",
              "  'property',\n",
              "  'pradesh',\n",
              "  'ppl',\n",
              "  'playing',\n",
              "  'pay',\n",
              "  'parole',\n",
              "  'pamela',\n",
              "  'pakistani',\n",
              "  'outrage',\n",
              "  'niggas',\n",
              "  'nagasaki',\n",
              "  'myanmar',\n",
              "  'muslims',\n",
              "  'mop',\n",
              "  'madhya',\n",
              "  'mad',\n",
              "  'lmao',\n",
              "  'learn',\n",
              "  'large',\n",
              "  'govt',\n",
              "  'give',\n",
              "  'gems',\n",
              "  'gave',\n",
              "  'funtenna',\n",
              "  'fukushima',\n",
              "  'former',\n",
              "  'film',\n",
              "  'earth',\n",
              "  'drive',\n",
              "  'downtown',\n",
              "  'dog',\n",
              "  'comes',\n",
              "  'closed',\n",
              "  'cake',\n",
              "  'british',\n",
              "  'bring',\n",
              "  'bbc',\n",
              "  'b',\n",
              "  'appears',\n",
              "  'aftershock',\n",
              "  '13',\n",
              "  '11',\n",
              "  'young',\n",
              "  'wow',\n",
              "  'worst',\n",
              "  'waving',\n",
              "  'washington',\n",
              "  'wanted',\n",
              "  'vs',\n",
              "  'view',\n",
              "  'upon',\n",
              "  'tweet',\n",
              "  'tree',\n",
              "  'tote',\n",
              "  'thousands',\n",
              "  'thinking',\n",
              "  'theater',\n",
              "  'soul',\n",
              "  'sky',\n",
              "  'sign',\n",
              "  'shows',\n",
              "  'shift',\n",
              "  'seeing',\n",
              "  'sea',\n",
              "  'scene',\n",
              "  'safety',\n",
              "  'rules',\n",
              "  'rock',\n",
              "  'reported',\n",
              "  'r',\n",
              "  'pray',\n",
              "  'playlist',\n",
              "  'patience',\n",
              "  ...],\n",
              " ['', '[UNK]', 'the', 'a', 'in'],\n",
              " ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1'])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating an embedding using an embedding layer\n",
        "\n",
        "To make our embedding, we're going to use the TF's embedding layer.\n",
        "\n",
        "The parameters we care the most about our embedding layers are:\n",
        "* input_dim= sie of our vocab\n",
        "* output_dim= size of the output embedding vector. A value of a 100 will mean that each token will be represented as a vector of 100 long length\n",
        "* input_length= length of sequences being passed to the embedding layer\n",
        "*"
      ],
      "metadata": {
        "id": "UscIzRK9b6M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "05ZNG325d5QR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding=layers.Embedding(input_dim=max_vocab_length,\n",
        "                           output_dim=128,\n",
        "                           input_length=max_length)"
      ],
      "metadata": {
        "id": "sQrtz6UzEDid"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFptstOWEVJj",
        "outputId": "9f38d2ac-0571-42f6-f891-9c7885c08852"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.core.embedding.Embedding at 0x7f20637dc1c0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get a random sentence from the training set\n",
        "random_sentence=random.choice(train_sentences)\n",
        "print(f\"Original Sentence:{random_sentence}\\\n",
        "      \\nEmbedded Sentence\")\n",
        "\n",
        "sample_embedding=embedding(text_vectorizer([random_sentence]))\n",
        "sample_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0XRCbb8EYo0",
        "outputId": "d7f8d13d-896a-4498-842e-81c16d9c4a76"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence:Our little forest fire wardens http://t.co/aPreNsss3x      \n",
            "Embedded Sentence\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[-0.01826651, -0.0455185 ,  0.04478427, ..., -0.04277081,\n",
              "         -0.02203444, -0.04961191],\n",
              "        [ 0.04756333, -0.04445172,  0.01160629, ..., -0.00764678,\n",
              "          0.04964563, -0.03272048],\n",
              "        [-0.01116407, -0.03684238,  0.02412797, ...,  0.0487757 ,\n",
              "          0.04302664, -0.02289302],\n",
              "        ...,\n",
              "        [-0.04026305, -0.01398071, -0.04515808, ..., -0.02679839,\n",
              "         -0.04787008,  0.02381423],\n",
              "        [-0.04026305, -0.01398071, -0.04515808, ..., -0.02679839,\n",
              "         -0.04787008,  0.02381423],\n",
              "        [-0.04026305, -0.01398071, -0.04515808, ..., -0.02679839,\n",
              "         -0.04787008,  0.02381423]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check out a single token's embedding\n",
        "sample_embedding[0][0], sample_embedding[0][0].shape, random_sentence[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12ur4eOWE9w1",
        "outputId": "3c3a4666-6cf6-4d2d-dea9-00aece4b45fb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              " array([-0.01826651, -0.0455185 ,  0.04478427, -0.03212946, -0.01990367,\n",
              "        -0.03452847,  0.02791091,  0.00127625, -0.04770733, -0.02936174,\n",
              "        -0.01973971,  0.01447019, -0.02952641, -0.02820892, -0.01930742,\n",
              "        -0.04286517,  0.04383672, -0.014073  ,  0.02084161, -0.03609059,\n",
              "        -0.01494258,  0.03745918,  0.02149117, -0.02346244,  0.02442988,\n",
              "         0.03049059,  0.03171021, -0.03821176,  0.02104301, -0.01027073,\n",
              "         0.031429  , -0.04826967, -0.02511449, -0.01356739,  0.04092226,\n",
              "         0.00390397, -0.00714183,  0.00134672,  0.03417296,  0.03022328,\n",
              "        -0.01120727,  0.03472864, -0.00395777,  0.02994481,  0.04266192,\n",
              "        -0.00809723, -0.00190679,  0.0080866 , -0.04014977, -0.03256259,\n",
              "        -0.02171372,  0.03067607, -0.03380334, -0.02108996,  0.04747155,\n",
              "         0.04911971,  0.00842319,  0.01081511,  0.03777042, -0.03020145,\n",
              "         0.04646881, -0.04006764,  0.03422506, -0.02774705,  0.00046519,\n",
              "         0.0494157 ,  0.01621049, -0.00977809, -0.03273425, -0.02559205,\n",
              "         0.04730966, -0.0073185 , -0.03654889,  0.00400065, -0.01698991,\n",
              "         0.01345548,  0.03581489, -0.04079108, -0.0473349 , -0.03044257,\n",
              "        -0.02255346,  0.019178  ,  0.00784011,  0.04040719, -0.03447578,\n",
              "        -0.02860018,  0.02503384,  0.04352672, -0.00391888,  0.03321422,\n",
              "         0.03437661, -0.03811719, -0.01387093, -0.00202484,  0.02011107,\n",
              "        -0.04995389,  0.01529874,  0.04631155, -0.02139196, -0.00915734,\n",
              "         0.04839385, -0.02336985,  0.04650554, -0.04839492,  0.03801035,\n",
              "         0.03356565,  0.0315439 , -0.00655861,  0.01982143, -0.01780521,\n",
              "         0.01965543, -0.01887186,  0.0015192 , -0.00892206, -0.02652814,\n",
              "         0.03631203,  0.0367671 , -0.04244242, -0.01505251,  0.03476001,\n",
              "        -0.01788186,  0.02294505, -0.02083345,  0.002659  , -0.01942372,\n",
              "        -0.04277081, -0.02203444, -0.04961191], dtype=float32)>,\n",
              " TensorShape([128]),\n",
              " 'O')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modelling a text dataset(running a series of experiments)\n",
        "\n",
        "* Model 0: Naive Bayes with Tokenization\n",
        "* Model 1: Feed-forward Neural Network(Dense Model)\n",
        "* Model 2: LSTM Model(RNN)\n",
        "* Model 3: GRU Model\n",
        "* Moel 4: Bidirectional LSTM\n",
        "* Model 5 : 1D Convolutional Layer\n",
        "* model 6: Tensorflow hub pretrained feature extraction (using transfer learning)\n",
        "* Model 7 : Same as model 6 with 10% of data\n",
        "\n",
        "How are we going to approach all of these:\n",
        "Use the standard steps in modelling with tensorflow\n",
        "\n",
        "* Create->build->fit->Evaluate\n",
        "\n"
      ],
      "metadata": {
        "id": "FhiG7RYnF6Le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 0\n",
        "\n",
        "To create a baseline, we'll use sklearn's Naive Bayes using Tf-IDF to convert our words to numbers\n",
        "\n",
        "It is a good practice to use non-DL Algos as a baseline because of their speed and then later use DL to see if you can improve upon them"
      ],
      "metadata": {
        "id": "aXUkwxQcI759"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "SgYwLWoxZ-rB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0=Pipeline([\n",
        "    (\"tfidf\",TfidfVectorizer()),\n",
        "    (\"clf\",MultinomialNB())\n",
        "])\n",
        "\n",
        "model_0.fit(train_sentences, train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "c2zhqUaGZ_un",
        "outputId": "bd397643-25b1-48fa-acb9-4583c06504c9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the baseline Model\n",
        "baseline_score = model_0.score(val_sentences, val_labels)\n",
        "print(f\"our baseline model achieves an accuracy of : {baseline_score*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqpo7Zw4cuEp",
        "outputId": "e627df5c-51d4-4f0c-9857-d093c6032dca"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our baseline model achieves an accuracy of : 79.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.target.value_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK5L1LTydGik",
        "outputId": "be78b9e9-33ed-41ed-f143-60eb3ac895f5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method IndexOpsMixin.value_counts of 0       1\n",
              "1       1\n",
              "2       1\n",
              "3       1\n",
              "4       1\n",
              "       ..\n",
              "7608    1\n",
              "7609    1\n",
              "7610    1\n",
              "7611    1\n",
              "7612    1\n",
              "Name: target, Length: 7613, dtype: int64>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Make predictions\n",
        "baseline_preds= model_0.predict(val_sentences)\n",
        "baseline_preds[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSf0XjLidIru",
        "outputId": "6fef47fe-dc36-47c5-d07a-fbfd8f01dcfa"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an evaluation function for our model experiments"
      ],
      "metadata": {
        "id": "f7C2BDI1dUx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could evaluate all of our model's predictions with different metrics every time.\n",
        "Calculate:\n",
        "* Accuracy\n",
        "* Precision\n",
        "* Recall\n",
        "* F1 score\n"
      ],
      "metadata": {
        "id": "fsfPdfPdeBYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to evaluate the above metrics\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculated model accuracy, precision, recall and f1 score of a binary classification model\n",
        "  \"\"\"\n",
        "  #Calculate model accuracy\n",
        "  model_accuracy=accuracy_score(y_true, y_pred) * 100\n",
        "  #Calculate model precision, recall and f1 score using the\"weighted average\"\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred,average=\"weighted\" )\n",
        "  model_results={'accuracy':model_accuracy,\n",
        "                 'precision': model_precision,\n",
        "                 'recall': model_recall,\n",
        "                 'f1-score':model_f1}\n",
        "  return model_results"
      ],
      "metadata": {
        "id": "w3KO2bNCeH9x"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get baseline results\n",
        "baseline_results=calculate_results(y_true=val_labels,\n",
        "                                   y_pred=baseline_preds)"
      ],
      "metadata": {
        "id": "ZRhBrZUceYKk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results['accuracy']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5u_ypVpggRH",
        "outputId": "b0029c51-7bea-44aa-fcaa-554f0381ad71"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79.26509186351706"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgSLKFUigjOp",
        "outputId": "3e8bd7c6-6124-45c4-de04-100f1f0d95ee"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1-score': 0.7862189758049549}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: A simple Dense Model"
      ],
      "metadata": {
        "id": "GkA8wqsZgkFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a tensorflow callback\n",
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "#Create a directory to save tensorboard logs\n",
        "SAVE_DIR=\"model_logs\"\n"
      ],
      "metadata": {
        "id": "4UMvTAO0lRnx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build model with functional API\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs=layers.Input(shape=(1,), dtype=tf.string) #Inputs are 1 dimensional\n",
        "x= text_vectorizer(inputs) #Turn the input text into numbers\n",
        "x= embedding(x) #Create an embedding of vctorized embeddings\n",
        "x=layers.GlobalAveragePooling1D()(x)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_1=tf.keras.Model(inputs, outputs, name=\"Model_1_dense\")"
      ],
      "metadata": {
        "id": "v-YXqNhHlXFp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGntW8rbmRG_",
        "outputId": "1f76c993-6e6c-4bb7-8530-7b10ff686695"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Model_1_dense\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 128)              0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,280,129\n",
            "Trainable params: 1,280,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile model\n",
        "model_1.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "AtH4W9MamYbc"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_history=model_1.fit(x=train_sentences,\n",
        "                            y=train_labels,\n",
        "                            epochs=5,\n",
        "                            validation_data=(val_sentences,val_labels),\n",
        "                            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
        "                                                                   experiment_name=\"model_1_dense\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG4M-QjhnJEQ",
        "outputId": "2fa2a3dd-247b-46b7-d9ea-960cbf00311a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/model_1_dense/20230616-021343\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 9s 36ms/step - loss: 0.6087 - accuracy: 0.6970 - val_loss: 0.5365 - val_accuracy: 0.7480\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 7s 33ms/step - loss: 0.4413 - accuracy: 0.8212 - val_loss: 0.4733 - val_accuracy: 0.7887\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 8s 38ms/step - loss: 0.3470 - accuracy: 0.8594 - val_loss: 0.4605 - val_accuracy: 0.7874\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 7s 33ms/step - loss: 0.2841 - accuracy: 0.8905 - val_loss: 0.4612 - val_accuracy: 0.7861\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 8s 36ms/step - loss: 0.2370 - accuracy: 0.9115 - val_loss: 0.4769 - val_accuracy: 0.7900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.evaluate(val_sentences, val_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWArT8vcnekL",
        "outputId": "1b8c6e79-fe85-42eb-a947-6853be0b9993"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.7900\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.47688421607017517, 0.7900262475013733]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_pred_probs=model_1.predict(val_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QoASaI1nn5Y",
        "outputId": "0dd9ac99-986d-450a-a8c8-a0563b4f270b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_pred_probs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYMwZRAMntde",
        "outputId": "39b03974-db4a-4318-c5a9-295ec3f3cde4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(762, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_results(val_labels, model_1_pred_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "x3zF16Kjon-Y",
        "outputId": "94cd3720-a9df-4948-c25b-d4445423430b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-29ba58bcbd72>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_1_pred_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-bf803e99326d>\u001b[0m in \u001b[0;36mcalculate_results\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \"\"\"\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#Calculate model accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mmodel_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;31m#Calculate model precision, recall and f1 score using the\"weighted average\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mmodel_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weighted\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     97\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DefvW6HlpEhM",
        "outputId": "ad32ff12-ff28-4658-da19-e7015627662c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
              "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
              "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
              "       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
              "       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
              "       1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
              "       1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
              "       0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
              "       1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
              "       1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
              "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Model_prediction Probabs into label formats\n",
        "model_1_preds=tf.squeeze(tf.round(model_1_pred_probs))"
      ],
      "metadata": {
        "id": "Siu3N9yupHrE"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5_rn_dtpMSZ",
        "outputId": "211b59cb-63d3-44bf-9e0f-b561010fdb24"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(762,), dtype=float32, numpy=\n",
              "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
              "       1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "       0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
              "       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
              "       1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
              "       0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
              "       0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
              "       0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
              "       0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
              "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
              "       0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
              "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results=calculate_results(y_true=val_labels,\n",
        "                                  y_pred=model_1_preds)"
      ],
      "metadata": {
        "id": "e4LzOiRYqp97"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbsaMJeNq0-H",
        "outputId": "78ca0492-94ee-4649-d6c1-994086e8aeee"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.00262467191601,\n",
              " 'precision': 0.795059849830577,\n",
              " 'recall': 0.7900262467191601,\n",
              " 'f1-score': 0.7870730838048355}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS3Y8aGrq2r3",
        "outputId": "9f202916-0c2e-4a4e-bb7a-27cb35945978"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1-score': 0.7862189758049549}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.array(list(model_1_results.values()))> np.array(list(baseline_results.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pSLUJ6Zq4a8",
        "outputId": "5590698f-351a-451c-d0e9-90e67835003d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False,  True])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the learned embeddings\n"
      ],
      "metadata": {
        "id": "xYmbF71CrAkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the vocabulary from the text vectorization layer\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()"
      ],
      "metadata": {
        "id": "3jyHcUrWtyhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_in_vocab), words_in_vocab[:10]"
      ],
      "metadata": {
        "id": "DjOpoiLUt4T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1 summary\n",
        "model_1.summary()"
      ],
      "metadata": {
        "id": "3iZQmS3ouAsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the weight matrix of the embedding layers\n",
        "#These are numerical representation of our numerical datawhich are trained for 5 epochs\n",
        "\n",
        "embed_weights=model_1.get_layer(\"embedding\").get_weights()[0]\n",
        "embed_weights"
      ],
      "metadata": {
        "id": "cY5xpTiVuHbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embed_weights.shape)"
      ],
      "metadata": {
        "id": "_IDMc9ssuec6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got the embedding matrix our model has learned to represent tokens, let's see how we can visualize it\n",
        "\n",
        "To do so, tensorflow has a tool called Projector"
      ],
      "metadata": {
        "id": "lxSNxbJPusKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Code below is adapted from: https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\n",
        "import io\n",
        "\n",
        "# Create output writers\n",
        "out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
        "out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
        "\n",
        "# Write embedding vectors and words to file\n",
        "for num, word in enumerate(words_in_vocab):\n",
        "  if num == 0:\n",
        "     continue # skip padding token\n",
        "  vec = embed_weights[num]\n",
        "  out_m.write(word + \"\\n\") # write words to file\n",
        "  out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "metadata": {
        "id": "AWIx-3ehvLck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download files locally to upload to Embedding Projector\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download(\"embedding_vectors.tsv\")\n",
        "  files.download(\"embedding_metadata.tsv\")"
      ],
      "metadata": {
        "id": "5RPlhbYAwOBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks(RNNs)\n",
        "\n",
        "the premise of the Recurrent Neural Networs is to use the representation of the previous input to aid the representation of the later input\n",
        "\n",
        "RNNs are used for sequence data (this example is a sequence of text)"
      ],
      "metadata": {
        "id": "CrZT_hGHyOfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 📖 Resources:\n",
        "\n",
        "MIT Deep Learning Lecture on Recurrent Neural Networks - explains the background of recurrent neural networks and introduces LSTMs.\n",
        "The Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy - demonstrates the power of RNN's with examples generating various sequences.\n",
        "Understanding LSTMs by Chris Olah - an in-depth (and technical) look at the mechanics of the LSTM cell, possibly the most popular RNN building block."
      ],
      "metadata": {
        "id": "K_J8CWsiCbyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: LSTM\n",
        "\n",
        "LSTM= long short term memory\n",
        "\n",
        "Our structure of an RNN Looks like this\n",
        "\n",
        "Input(text)-> Tokenize -> Embedding -> Layers(RNNs/Dense)->output(label probability)"
      ],
      "metadata": {
        "id": "JIpS5KdMDdhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an LSTM Model\n",
        "from tensorflow.keras import layers\n",
        "inputs= layers.Input(shape=(1,), dtype=tf.string)\n",
        "x=text_vectorizer(inputs)\n",
        "x= embedding(x)\n",
        "print(x.shape)\n",
        "x=layers.LSTM(64, return_sequences=True)(x) #64 is hidden units\n",
        "#When you a stacking RNN Cells together, you need to return sequences=true\n",
        "print(x.shape)\n",
        "x= layers.LSTM(64)(x)\n",
        "print(x.shape)\n",
        "x= layers.Dense(64, activation='relu')(x)\n",
        "print(x.shape)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_2= tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA922aKJASoU",
        "outputId": "1cd180b9-bf9b-45be-f332-2af9effdedf9"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 15, 128)\n",
            "(None, 15, 64)\n",
            "(None, 64)\n",
            "(None, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an LSTM Model\n",
        "from tensorflow.keras import layers\n",
        "inputs= layers.Input(shape=(1,), dtype=tf.string)\n",
        "x=text_vectorizer(inputs)\n",
        "x= embedding(x)\n",
        "#print(x.shape)\n",
        "#x=layers.LSTM(64, return_sequences=True)(x) #64 is hidden units\n",
        "#When you a stacking RNN Cells together, you need to return sequences=true\n",
        "#print(x.shape)\n",
        "x= layers.LSTM(64)(x)\n",
        "#print(x.shape)\n",
        "#x= layers.Dense(64, activation='relu')(x)\n",
        "#print(x.shape)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_2= tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")\n"
      ],
      "metadata": {
        "id": "7nHB-E4PBuQU"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzlIZIY8Dm7Q",
        "outputId": "267ed951-8c04-42d5-e560-1ccca8b27641"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2_LSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       multiple                  1280000   \n",
            "                                                                 \n",
            " lstm_12 (LSTM)              (None, 64)                49408     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,329,473\n",
            "Trainable params: 1,329,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The default activation function for RNNs is TanH"
      ],
      "metadata": {
        "id": "QxsN2GykDo_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Complie the model\n",
        "model_2.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "rf9WWNf9D1xh"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_model_2_LSTM=model_2.fit(train_sentences,\n",
        "                                 train_labels,\n",
        "                                 epochs=5,\n",
        "                                 validation_data=(val_sentences, val_labels),\n",
        "                                 callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
        "                                                                        experiment_name=\"model_2_LSTM\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfH-KOfKEI_Z",
        "outputId": "36ed2b87-b06c-4d13-c4a7-9614b922e53d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/model_2_LSTM/20230616-023831\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 12s 43ms/step - loss: 0.2179 - accuracy: 0.9232 - val_loss: 0.5918 - val_accuracy: 0.7848\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 8s 36ms/step - loss: 0.1578 - accuracy: 0.9400 - val_loss: 0.5878 - val_accuracy: 0.7795\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 11s 49ms/step - loss: 0.1283 - accuracy: 0.9508 - val_loss: 0.8088 - val_accuracy: 0.7835\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 12s 56ms/step - loss: 0.1063 - accuracy: 0.9594 - val_loss: 0.8088 - val_accuracy: 0.7730\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 12s 55ms/step - loss: 0.0887 - accuracy: 0.9653 - val_loss: 0.9141 - val_accuracy: 0.7782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_pred_probs=model_2.predict(val_sentences)\n",
        "model_2_pred_probs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5qVJNf2EY7i",
        "outputId": "c2436ffb-5a48-4852-a6af-a971b465d642"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 2s 8ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.8220157e-03],\n",
              "       [4.7423866e-01],\n",
              "       [9.9969208e-01],\n",
              "       [4.2027272e-02],\n",
              "       [3.5633449e-04],\n",
              "       [9.9869198e-01],\n",
              "       [6.2313449e-01],\n",
              "       [9.9983722e-01],\n",
              "       [9.9966562e-01],\n",
              "       [5.9080404e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the probabs to pred\n",
        "model_2_preds=tf.squeeze(tf.round(model_2_pred_probs))"
      ],
      "metadata": {
        "id": "ZSymwXH5EjXO"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_preds\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfA-YNydEuin",
        "outputId": "939ba1d3-e7e7-4315-f138-e19bfd1fb128"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(762,), dtype=float32, numpy=\n",
              "array([0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "       0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
              "       1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
              "       1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
              "       0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
              "       0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
              "       0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
              "       0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
              "       0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
              "       0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
              "       0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
              "       0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
              "       0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
              "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
              "       1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_results=calculate_results(y_true=val_labels,\n",
        "                                  y_pred=model_2_preds)"
      ],
      "metadata": {
        "id": "F4Nt6h5PEwPr"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRUgB8tnE2Wz",
        "outputId": "dafb468d-86b9-4250-b39b-8918778cc5e0"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 77.82152230971128,\n",
              " 'precision': 0.7804499491198106,\n",
              " 'recall': 0.7782152230971129,\n",
              " 'f1-score': 0.7760126933653841}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y-bd5d6E3Mp",
        "outputId": "84161463-3aba-4a31-8c58-bb7278f1da41"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1-score': 0.7862189758049549}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: GRU\n",
        "\n",
        "Similar to LSTM but has less params"
      ],
      "metadata": {
        "id": "UXF740ofE4nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "inputs= tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "\n",
        "x=embedding(x)\n",
        "\n",
        "x = tf.keras.layers.GRU(64, return_sequences=True)(x) #if you want to stack recurrent layers on top of each other, you need to use return_sequences=True\n",
        "x=layers.LSTM(64, return_sequences=True)(x)\n",
        "x=layers.GRU(64)(x)\n",
        "\n",
        "x=layers.Dense(64, activation='relu')(x)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(x)\n",
        "model_3=tf.keras.Model(inputs, outputs, name=\"Model_3_GRU\")\n"
      ],
      "metadata": {
        "id": "co1zgceRGAbg"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "inputs= tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "\n",
        "x=embedding(x)\n",
        "\n",
        "x = tf.keras.layers.GRU(64)(x) #if you want to stack recurrent layers on top of each other, you need to use return_sequences=True\n",
        "#x=layers.LSTM(64, return_sequences=True)(x)\n",
        "#x=layers.GRU(64)(x)\n",
        "\n",
        "x=layers.Dense(64, activation='relu')(x)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(x)\n",
        "model_3=tf.keras.Model(inputs, outputs, name=\"Model_3_GRU\")\n"
      ],
      "metadata": {
        "id": "DY1pSYYhIGWy"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KKTktRsHIl3",
        "outputId": "fa3eb1d1-9066-4a0c-a199-f8b9565d223f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Model_3_GRU\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_15 (InputLayer)       [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       multiple                  1280000   \n",
            "                                                                 \n",
            " gru_7 (GRU)                 (None, 64)                37248     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,321,473\n",
            "Trainable params: 1,321,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "j-bjh1F5HkL7"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_model_3_GRU=model_3.fit(train_sentences,\n",
        "                                train_labels,\n",
        "                                validation_data=(val_sentences, val_labels),\n",
        "                                epochs=5,\n",
        "                                callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
        "                                                                      experiment_name='model_name_GRU')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCnE4GUYIBBe",
        "outputId": "837e7ff4-b4c6-4375-e81b-e83d7097befa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/model_name_GRU/20230616-025921\n",
            "Epoch 1/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2bOgGL-qJHvD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}