{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8atDvHWEcSJf1sI6Xh6Tg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshitgosain/Tensorflow-ZTM/blob/main/TF_ZTM_07_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Fundamentals in TF"
      ],
      "metadata": {
        "id": "CKP8rK-46aDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence-to-sequence\n",
        "* One to one- Image Captioning\n",
        "* Many to one- Sentiment analysis\n",
        "* Many to one- time series forecasting\n",
        "* many- to-many- Machine translation\n"
      ],
      "metadata": {
        "id": "UEIfyi-r8kcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfEm1Kes8lgu",
        "outputId": "4f7b6f14-e413-44ee-b31d-a1bef16d2edb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-ac9eb8d1-7ffa-373f-0123-bc43270c9b3a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3FIIV0NRezo",
        "outputId": "2350f6a8-d48a-4787-eb87-1f6d67f7dca3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-15 07:54:54--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-15 07:54:54 (87.9 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
      ],
      "metadata": {
        "id": "FZdua_xCR3nj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu1aEiChSBoa",
        "outputId": "7865e7ea-538b-4e6d-93bb-8a30a4911c65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-15 07:54:58--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.214.128, 172.253.114.128, 172.253.119.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.214.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp_getting_started.zip’\n",
            "\n",
            "\rnlp_getting_started   0%[                    ]       0  --.-KB/s               \rnlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-06-15 07:54:58 (84.3 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unzip_data(\"nlp_getting_started.zip\")"
      ],
      "metadata": {
        "id": "ZwhaaRJFSXGF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the dataset\n",
        "\n",
        "To visualize the sample data, we need to read the in. On way is to use python.\n",
        "\n",
        "Another way to do this is use pandas\n",
        "\n"
      ],
      "metadata": {
        "id": "FnODZX-2SZtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df=pd.read_csv(\"train.csv\")\n",
        "test_df=pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "OA7Fu5nwKZJE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yECy1rYFLgUa",
        "outputId": "b6d960eb-54fa-4e3d-fe46-dc879b99f87c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-26604478-6144-4c5b-8e27-074aac248044\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26604478-6144-4c5b-8e27-074aac248044')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-26604478-6144-4c5b-8e27-074aac248044 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-26604478-6144-4c5b-8e27-074aac248044');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_shuffled=train_df.sample(frac=1, random_state=42)"
      ],
      "metadata": {
        "id": "6mcpKv3ULfok"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_shuffled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "rheeFHeLMEPe",
        "outputId": "330c388d-b8f0-40fc-e5e9-f4c8411d9357"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id       keyword                        location  \\\n",
              "2644   3796   destruction                             NaN   \n",
              "2227   3185        deluge                             NaN   \n",
              "5448   7769        police                              UK   \n",
              "132     191    aftershock                             NaN   \n",
              "6845   9810        trauma           Montgomery County, MD   \n",
              "...     ...           ...                             ...   \n",
              "5226   7470  obliteration                         Merica!   \n",
              "5390   7691         panic                             NaN   \n",
              "860    1242         blood                             NaN   \n",
              "7603  10862           NaN                             NaN   \n",
              "7270  10409     whirlwind  Stamford & Cork (& Shropshire)   \n",
              "\n",
              "                                                   text  target  \n",
              "2644  So you have a new weapon that can cause un-ima...       1  \n",
              "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
              "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
              "132   Aftershock back to school kick off was great. ...       0  \n",
              "6845  in response to trauma Children of Addicts deve...       0  \n",
              "...                                                 ...     ...  \n",
              "5226  @Eganator2000 There aren't many Obliteration s...       0  \n",
              "5390  just had a panic attack bc I don't have enough...       0  \n",
              "860   Omron HEM-712C Automatic Blood Pressure Monito...       0  \n",
              "7603  Officials say a quarantine is in place at an A...       1  \n",
              "7270  I moved to England five years ago today. What ...       1  \n",
              "\n",
              "[7613 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9298af38-2b67-44a6-b7ce-0d76f4b5b5ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>3796</td>\n",
              "      <td>destruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>So you have a new weapon that can cause un-ima...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>3185</td>\n",
              "      <td>deluge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>7769</td>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>191</td>\n",
              "      <td>aftershock</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Aftershock back to school kick off was great. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>9810</td>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts deve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5226</th>\n",
              "      <td>7470</td>\n",
              "      <td>obliteration</td>\n",
              "      <td>Merica!</td>\n",
              "      <td>@Eganator2000 There aren't many Obliteration s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>7691</td>\n",
              "      <td>panic</td>\n",
              "      <td>NaN</td>\n",
              "      <td>just had a panic attack bc I don't have enough...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>1242</td>\n",
              "      <td>blood</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Omron HEM-712C Automatic Blood Pressure Monito...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7603</th>\n",
              "      <td>10862</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Officials say a quarantine is in place at an A...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7270</th>\n",
              "      <td>10409</td>\n",
              "      <td>whirlwind</td>\n",
              "      <td>Stamford &amp; Cork (&amp; Shropshire)</td>\n",
              "      <td>I moved to England five years ago today. What ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7613 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9298af38-2b67-44a6-b7ce-0d76f4b5b5ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9298af38-2b67-44a6-b7ce-0d76f4b5b5ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9298af38-2b67-44a6-b7ce-0d76f4b5b5ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test dataframe looks like:\n",
        "test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EOKnKXMgMLmz",
        "outputId": "601b4d2f-ec54-4c03-994e-c5adc8499f5d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0cded96-e317-4692-bafd-51e15f56a659\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0cded96-e317-4692-bafd-51e15f56a659')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b0cded96-e317-4692-bafd-51e15f56a659 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b0cded96-e317-4692-bafd-51e15f56a659');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVroyArwMSAd",
        "outputId": "3d7068f6-3bfc-4f1e-8dc9-e3c8073429c8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#How many total number of samples\n",
        "len(train_df), len(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJwjZWDvMbP0",
        "outputId": "fc41abb3-61c6-41da-f5c7-30ce65198941"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7613, 3263)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's visualize some random training samples\n",
        "import random\n",
        "random_index = random.randint(0,len(train_df)-5)\n",
        "for row in train_data_shuffled[['text','target']][random_index:random_index+5].itertuples():\n",
        "  _,text, target=row\n",
        "  print(f\"Target: {target}\", \"(real disaster)\" if target >0 else \"(not real disaster)\")\n",
        "  print(f\"text:\\n{text}\\n\")\n",
        "  print(\"---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDoIh77fM8xb",
        "outputId": "f8f1f7b2-65f3-44bb-d06b-7b0908bb13d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target: 0 (not real disaster)\n",
            "text:\n",
            "@annajhm @JCOMANSE @paul_staubs @rslm72254 @blanktgt You must've got in trouble ??that's why you have #Fartanxiety now????\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "text:\n",
            "@115Film Doctor we must leave immediately the Core is unstable...The whole building is told to be evacuated. Take the research. We need...\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "text:\n",
            "@irishspy What you don't think the Allies should have just sucked up 1 million casualties?\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "text:\n",
            "People are more worried about the burning of buildings than black people losing their lives disgusting.\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "text:\n",
            "Only one man Tsutomi Yamaguchi is said to have survived both atomic bomb blasts at #Hiroshima and #Nagasaki. #OTD http://t.co/DaalPeNZp0\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data into training and validation sets\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3_KYNkVORfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "K-_DDtcAeap3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, val_sentences, train_labels, val_labels=train_test_split(train_data_shuffled['text'].to_numpy(),\n",
        "                                                                          train_data_shuffled['target'].to_numpy(),\n",
        "                                                                          test_size=0.1,\n",
        "                                                                          random_state=42)"
      ],
      "metadata": {
        "id": "rFRtXZeded8h"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the lengths\n",
        "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvV1nQX6fD8S",
        "outputId": "2ed3dbd8-de7f-468a-b273-cca2bc7759c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6851, 6851, 762, 762)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting words into numbers\n",
        "\n",
        "First thing while building a model is to convert your text into numbers\n"
      ],
      "metadata": {
        "id": "xQJN3MYYktnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization VS Embedding\n",
        "Tkenization - word level and character level. Direct apping of a token.\n",
        "\n",
        "Embedding - every word gets turned into a vector and we can define size of the vector. Embeddings can learn as our model trains\n",
        "\n"
      ],
      "metadata": {
        "id": "6GOhJ-mcfRFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text vectorization(tokenization)"
      ],
      "metadata": {
        "id": "CZ_FBHbwjhjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "metadata": {
        "id": "otEJGiTzJmNX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the default text vectorization parameters\n",
        "text_vectoizer= TextVectorization(max_tokens=None, #Defines how many words in our vocab\n",
        "                               standardize= \"lower_and_strip_punctuation\",\n",
        "                               split=\"whitespace\",\n",
        "                               ngrams=None,\n",
        "                               output_mode='int',\n",
        "                               output_sequence_length=None) #how long do you want your sequences to be\n",
        "                               #pad_to_max_tokens=True)"
      ],
      "metadata": {
        "id": "1iQnXkkeJvaQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[0].split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7P5w8FmXiRk",
        "outputId": "f647e157-64ce-49d2-97ef-5251659399e3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@mogacola', '@zamtriossu', 'i', 'screamed', 'after', 'hitting', 'tweet']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the average number of tokens(words) in the training tweets\n",
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUNdYmJ9WjZY",
        "outputId": "16937329-c714-460c-c91d-e5dee94515b8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_length= 10000 #max no of words to have in our vocab\n",
        "max_length=15 #Max length our sequences will be (e.g. how many words from a Tweet does a model see)\n",
        "\n",
        "text_vectorizer=TextVectorization(max_tokens=max_vocab_length,\n",
        "                                     output_mode=\"int\",\n",
        "                                     output_sequence_length=max_length)"
      ],
      "metadata": {
        "id": "LMx1h8VWX4uS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the text vectorizer to the training text\n",
        "text_vectorizer.adapt(train_sentences)\n"
      ],
      "metadata": {
        "id": "yo-nO5JSYXiy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIUXVbetZ-sI",
        "outputId": "3ecac6af-3a2e-40f0-a4cd-0ec2b9aea06f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "       'Imagine getting flattened by Kurt Zouma',\n",
              "       '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "       \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "       'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
              "       '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
              "       'destroy the free fandom honestly',\n",
              "       'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
              "       '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
              "       'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a sample sentence and tokenize it\n",
        "sample_sentence=\"There is a floow in my street!\"\n",
        "text_vectorizer([sample_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfFwEaKYaAoe",
        "outputId": "cb4ec5c1-85bd-465d-ce8b-3caab50d1d7d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[ 74,   9,   3,   1,   4,  13, 698,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_sentence=random.choice(train_sentences)\n",
        "print(random_sentence)\n",
        "vectorized_random_sentence=text_vectorizer([random_sentence])\n",
        "print(f\"Random Sentence: {random_sentence}\\nVectoized Random Sentence: {vectorized_random_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfpgC8tgaN4z",
        "outputId": "87f40687-5937-4b33-902c-62d440a65ca0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Afghanistan: sharp rise in women and children casualties in first half of #2015 http://t.co/LdyWd4ydT9\n",
            "Random Sentence: #Afghanistan: sharp rise in women and children casualties in first half of #2015 http://t.co/LdyWd4ydT9\n",
            "Vectoized Random Sentence: [[1439 2300  727    4  427    7  656  518    4   97  575    6  208    1\n",
            "     0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_vocab=text_vectorizer.get_vocabulary()\n",
        "top_5_words=words_in_vocab[:5]\n",
        "botton_5_words=words_in_vocab[-5:]"
      ],
      "metadata": {
        "id": "ijV2vvTqbNLm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_vocab,top_5_words,botton_5_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WMshKSybv4a",
        "outputId": "2f3308a2-ed7c-483c-94a9-4b1f485e406e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['',\n",
              "  '[UNK]',\n",
              "  'the',\n",
              "  'a',\n",
              "  'in',\n",
              "  'to',\n",
              "  'of',\n",
              "  'and',\n",
              "  'i',\n",
              "  'is',\n",
              "  'for',\n",
              "  'on',\n",
              "  'you',\n",
              "  'my',\n",
              "  'with',\n",
              "  'it',\n",
              "  'that',\n",
              "  'at',\n",
              "  'by',\n",
              "  'this',\n",
              "  'from',\n",
              "  'be',\n",
              "  'are',\n",
              "  'was',\n",
              "  'have',\n",
              "  'like',\n",
              "  'as',\n",
              "  'up',\n",
              "  'so',\n",
              "  'just',\n",
              "  'but',\n",
              "  'me',\n",
              "  'im',\n",
              "  'your',\n",
              "  'not',\n",
              "  'amp',\n",
              "  'out',\n",
              "  'its',\n",
              "  'will',\n",
              "  'an',\n",
              "  'no',\n",
              "  'has',\n",
              "  'fire',\n",
              "  'after',\n",
              "  'all',\n",
              "  'when',\n",
              "  'we',\n",
              "  'if',\n",
              "  'now',\n",
              "  'via',\n",
              "  'new',\n",
              "  'more',\n",
              "  'get',\n",
              "  'or',\n",
              "  'about',\n",
              "  'what',\n",
              "  'he',\n",
              "  'people',\n",
              "  'news',\n",
              "  'been',\n",
              "  'over',\n",
              "  'one',\n",
              "  'how',\n",
              "  'dont',\n",
              "  'they',\n",
              "  'who',\n",
              "  'into',\n",
              "  'were',\n",
              "  'do',\n",
              "  'us',\n",
              "  '2',\n",
              "  'can',\n",
              "  'video',\n",
              "  'emergency',\n",
              "  'there',\n",
              "  'disaster',\n",
              "  'than',\n",
              "  'police',\n",
              "  'would',\n",
              "  'his',\n",
              "  'still',\n",
              "  'her',\n",
              "  'some',\n",
              "  'body',\n",
              "  'storm',\n",
              "  'crash',\n",
              "  'burning',\n",
              "  'suicide',\n",
              "  'back',\n",
              "  'man',\n",
              "  'california',\n",
              "  'why',\n",
              "  'time',\n",
              "  'them',\n",
              "  'had',\n",
              "  'buildings',\n",
              "  'rt',\n",
              "  'first',\n",
              "  'cant',\n",
              "  'see',\n",
              "  'got',\n",
              "  'day',\n",
              "  'off',\n",
              "  'our',\n",
              "  'going',\n",
              "  'nuclear',\n",
              "  'know',\n",
              "  'world',\n",
              "  'bomb',\n",
              "  'fires',\n",
              "  'love',\n",
              "  'killed',\n",
              "  'go',\n",
              "  'attack',\n",
              "  'youtube',\n",
              "  'dead',\n",
              "  'two',\n",
              "  'families',\n",
              "  '3',\n",
              "  'train',\n",
              "  'full',\n",
              "  'being',\n",
              "  'war',\n",
              "  'many',\n",
              "  'today',\n",
              "  'think',\n",
              "  'only',\n",
              "  'car',\n",
              "  'accident',\n",
              "  'life',\n",
              "  'hiroshima',\n",
              "  'their',\n",
              "  'say',\n",
              "  'may',\n",
              "  'down',\n",
              "  'watch',\n",
              "  'good',\n",
              "  'could',\n",
              "  'want',\n",
              "  'last',\n",
              "  'here',\n",
              "  'years',\n",
              "  'u',\n",
              "  'then',\n",
              "  'make',\n",
              "  'did',\n",
              "  'wildfire',\n",
              "  'way',\n",
              "  'help',\n",
              "  'best',\n",
              "  'too',\n",
              "  'even',\n",
              "  'because',\n",
              "  'home',\n",
              "  'death',\n",
              "  'collapse',\n",
              "  'bombing',\n",
              "  'mass',\n",
              "  'him',\n",
              "  'black',\n",
              "  'am',\n",
              "  'those',\n",
              "  'need',\n",
              "  'fatal',\n",
              "  'army',\n",
              "  'another',\n",
              "  'work',\n",
              "  'take',\n",
              "  'should',\n",
              "  'really',\n",
              "  'please',\n",
              "  'mh370',\n",
              "  'youre',\n",
              "  'look',\n",
              "  'lol',\n",
              "  'hot',\n",
              "  'pm',\n",
              "  'legionnaires',\n",
              "  '4',\n",
              "  'right',\n",
              "  '5',\n",
              "  'let',\n",
              "  'city',\n",
              "  'year',\n",
              "  'wreck',\n",
              "  'school',\n",
              "  'northern',\n",
              "  'much',\n",
              "  'forest',\n",
              "  'bomber',\n",
              "  'water',\n",
              "  'she',\n",
              "  'never',\n",
              "  'read',\n",
              "  'latest',\n",
              "  'homes',\n",
              "  'great',\n",
              "  'every',\n",
              "  '1',\n",
              "  'live',\n",
              "  'god',\n",
              "  'fear',\n",
              "  'any',\n",
              "  '\\x89Û',\n",
              "  'under',\n",
              "  'said',\n",
              "  'old',\n",
              "  'floods',\n",
              "  '2015',\n",
              "  'getting',\n",
              "  'atomic',\n",
              "  'while',\n",
              "  'top',\n",
              "  'obama',\n",
              "  'feel',\n",
              "  'thats',\n",
              "  'since',\n",
              "  'near',\n",
              "  'flames',\n",
              "  'ever',\n",
              "  'come',\n",
              "  'where',\n",
              "  'these',\n",
              "  'military',\n",
              "  'japan',\n",
              "  'found',\n",
              "  'content',\n",
              "  'ass',\n",
              "  'without',\n",
              "  'weather',\n",
              "  'most',\n",
              "  'flooding',\n",
              "  'flood',\n",
              "  'damage',\n",
              "  'which',\n",
              "  'shit',\n",
              "  's',\n",
              "  'hope',\n",
              "  'everyone',\n",
              "  'before',\n",
              "  'stop',\n",
              "  'plan',\n",
              "  'malaysia',\n",
              "  'injured',\n",
              "  'hit',\n",
              "  'evacuation',\n",
              "  'during',\n",
              "  'debris',\n",
              "  'cross',\n",
              "  'coming',\n",
              "  'wild',\n",
              "  'well',\n",
              "  'times',\n",
              "  'sinking',\n",
              "  'oil',\n",
              "  'fucking',\n",
              "  'check',\n",
              "  'cause',\n",
              "  'weapons',\n",
              "  'truck',\n",
              "  'food',\n",
              "  'bloody',\n",
              "  'always',\n",
              "  'weapon',\n",
              "  'theres',\n",
              "  'state',\n",
              "  'little',\n",
              "  'injuries',\n",
              "  'free',\n",
              "  'wounded',\n",
              "  'summer',\n",
              "  'smoke',\n",
              "  'severe',\n",
              "  'reddit',\n",
              "  'next',\n",
              "  'movie',\n",
              "  'ive',\n",
              "  'hes',\n",
              "  'fall',\n",
              "  'evacuate',\n",
              "  'confirmed',\n",
              "  'bad',\n",
              "  'again',\n",
              "  'thunderstorm',\n",
              "  'set',\n",
              "  'night',\n",
              "  'natural',\n",
              "  'looks',\n",
              "  'heat',\n",
              "  'face',\n",
              "  'earthquake',\n",
              "  'boy',\n",
              "  'whole',\n",
              "  'until',\n",
              "  'thunder',\n",
              "  'through',\n",
              "  'says',\n",
              "  'panic',\n",
              "  'outbreak',\n",
              "  'made',\n",
              "  'lightning',\n",
              "  'fatalities',\n",
              "  'family',\n",
              "  'explosion',\n",
              "  'end',\n",
              "  'destroy',\n",
              "  'derailment',\n",
              "  'air',\n",
              "  'w',\n",
              "  'terrorist',\n",
              "  'survive',\n",
              "  'screaming',\n",
              "  'saudi',\n",
              "  'refugees',\n",
              "  'rain',\n",
              "  'murder',\n",
              "  'loud',\n",
              "  'liked',\n",
              "  'house',\n",
              "  'gonna',\n",
              "  'failure',\n",
              "  'collided',\n",
              "  'bag',\n",
              "  'attacked',\n",
              "  'ambulance',\n",
              "  '70',\n",
              "  'wind',\n",
              "  'services',\n",
              "  'save',\n",
              "  'report',\n",
              "  'migrants',\n",
              "  'head',\n",
              "  'explode',\n",
              "  'charged',\n",
              "  'change',\n",
              "  'big',\n",
              "  'also',\n",
              "  'wrecked',\n",
              "  'warning',\n",
              "  'update',\n",
              "  'run',\n",
              "  'rescuers',\n",
              "  'released',\n",
              "  'photo',\n",
              "  'massacre',\n",
              "  'injury',\n",
              "  'hurricane',\n",
              "  'high',\n",
              "  'hail',\n",
              "  'fuck',\n",
              "  'does',\n",
              "  'destroyed',\n",
              "  'bus',\n",
              "  'blood',\n",
              "  '40',\n",
              "  '\\x89ÛÒ',\n",
              "  'wreckage',\n",
              "  'violent',\n",
              "  'twister',\n",
              "  'trauma',\n",
              "  'tragedy',\n",
              "  'terrorism',\n",
              "  'survivors',\n",
              "  'survived',\n",
              "  'sinkhole',\n",
              "  'sandstorm',\n",
              "  'road',\n",
              "  'rioting',\n",
              "  'red',\n",
              "  'real',\n",
              "  'put',\n",
              "  'post',\n",
              "  'national',\n",
              "  'missing',\n",
              "  'landslide',\n",
              "  'keep',\n",
              "  'girl',\n",
              "  'drought',\n",
              "  'curfew',\n",
              "  'breaking',\n",
              "  'bags',\n",
              "  'white',\n",
              "  'twitter',\n",
              "  'tonight',\n",
              "  'structural',\n",
              "  'spill',\n",
              "  'service',\n",
              "  'screamed',\n",
              "  'rescued',\n",
              "  'rescue',\n",
              "  'phone',\n",
              "  'ok',\n",
              "  'oh',\n",
              "  'mosque',\n",
              "  'lives',\n",
              "  'horrible',\n",
              "  'harm',\n",
              "  'game',\n",
              "  'dust',\n",
              "  'destruction',\n",
              "  'deluge',\n",
              "  'deaths',\n",
              "  'crashed',\n",
              "  'cliff',\n",
              "  'catastrophe',\n",
              "  'boat',\n",
              "  'away',\n",
              "  'august',\n",
              "  'area',\n",
              "  'apocalypse',\n",
              "  'woman',\n",
              "  'whirlwind',\n",
              "  'traumatised',\n",
              "  'stock',\n",
              "  'saw',\n",
              "  'ruin',\n",
              "  'riot',\n",
              "  'quarantine',\n",
              "  'kills',\n",
              "  'island',\n",
              "  'investigators',\n",
              "  'ill',\n",
              "  'hostages',\n",
              "  'hazard',\n",
              "  'danger',\n",
              "  'call',\n",
              "  '15',\n",
              "  'women',\n",
              "  'windstorm',\n",
              "  'things',\n",
              "  'suspect',\n",
              "  'show',\n",
              "  'reunion',\n",
              "  'quarantined',\n",
              "  'lava',\n",
              "  'heart',\n",
              "  'engulfed',\n",
              "  'detonate',\n",
              "  'crush',\n",
              "  'collapsed',\n",
              "  'came',\n",
              "  'better',\n",
              "  'battle',\n",
              "  'armageddon',\n",
              "  'airplane',\n",
              "  'against',\n",
              "  'affected',\n",
              "  'use',\n",
              "  'trapped',\n",
              "  'thank',\n",
              "  'sunk',\n",
              "  'story',\n",
              "  'send',\n",
              "  'part',\n",
              "  'other',\n",
              "  'must',\n",
              "  'mudslide',\n",
              "  'market',\n",
              "  'iran',\n",
              "  'famine',\n",
              "  'exploded',\n",
              "  'electrocuted',\n",
              "  'ebay',\n",
              "  'displaced',\n",
              "  'derailed',\n",
              "  'derail',\n",
              "  'burned',\n",
              "  'bombed',\n",
              "  'blown',\n",
              "  'baby',\n",
              "  'around',\n",
              "  'zone',\n",
              "  'wave',\n",
              "  'wanna',\n",
              "  'sure',\n",
              "  'someone',\n",
              "  'screams',\n",
              "  'razed',\n",
              "  'power',\n",
              "  'obliterated',\n",
              "  'long',\n",
              "  'land',\n",
              "  'hundreds',\n",
              "  'heard',\n",
              "  'group',\n",
              "  'flattened',\n",
              "  'drown',\n",
              "  'doing',\n",
              "  'care',\n",
              "  'bridge',\n",
              "  'bagging',\n",
              "  '9',\n",
              "  'went',\n",
              "  'used',\n",
              "  'typhoon',\n",
              "  'trouble',\n",
              "  'tornado',\n",
              "  'thought',\n",
              "  'thing',\n",
              "  'river',\n",
              "  'responders',\n",
              "  'past',\n",
              "  'pandemonium',\n",
              "  'officials',\n",
              "  'meltdown',\n",
              "  'lot',\n",
              "  'least',\n",
              "  'inundated',\n",
              "  'id',\n",
              "  'hostage',\n",
              "  'hijacking',\n",
              "  'hazardous',\n",
              "  'goes',\n",
              "  'drowning',\n",
              "  'didnt',\n",
              "  'devastation',\n",
              "  'demolish',\n",
              "  'collide',\n",
              "  'casualties',\n",
              "  'calgary',\n",
              "  'bang',\n",
              "  'anniversary',\n",
              "  'yet',\n",
              "  'wounds',\n",
              "  'volcano',\n",
              "  'tsunami',\n",
              "  'sue',\n",
              "  'st',\n",
              "  'song',\n",
              "  'something',\n",
              "  'shoulder',\n",
              "  'security',\n",
              "  'prebreak',\n",
              "  'possible',\n",
              "  'pkk',\n",
              "  'panicking',\n",
              "  'obliteration',\n",
              "  'obliterate',\n",
              "  'murderer',\n",
              "  'minute',\n",
              "  'light',\n",
              "  'lets',\n",
              "  'kill',\n",
              "  'isis',\n",
              "  'india',\n",
              "  'hijacker',\n",
              "  'hellfire',\n",
              "  'government',\n",
              "  'few',\n",
              "  'evacuated',\n",
              "  'due',\n",
              "  'detonated',\n",
              "  'desolation',\n",
              "  'crushed',\n",
              "  'chemical',\n",
              "  'blew',\n",
              "  'blazing',\n",
              "  'blast',\n",
              "  'annihilated',\n",
              "  'airport',\n",
              "  '6',\n",
              "  'week',\n",
              "  'upheaval',\n",
              "  'trying',\n",
              "  'three',\n",
              "  'thanks',\n",
              "  'sound',\n",
              "  'soon',\n",
              "  'sirens',\n",
              "  'rainstorm',\n",
              "  'plane',\n",
              "  'music',\n",
              "  'making',\n",
              "  'kids',\n",
              "  'issues',\n",
              "  'half',\n",
              "  'guys',\n",
              "  'fedex',\n",
              "  'done',\n",
              "  'died',\n",
              "  'detonation',\n",
              "  'days',\n",
              "  'cyclone',\n",
              "  'county',\n",
              "  'collision',\n",
              "  'caused',\n",
              "  'catastrophic',\n",
              "  'bleeding',\n",
              "  'beautiful',\n",
              "  '8',\n",
              "  'words',\n",
              "  'very',\n",
              "  'traffic',\n",
              "  'south',\n",
              "  'remember',\n",
              "  'policy',\n",
              "  'place',\n",
              "  'nothing',\n",
              "  'north',\n",
              "  'mp',\n",
              "  'longer',\n",
              "  'left',\n",
              "  'israeli',\n",
              "  'hell',\n",
              "  'fun',\n",
              "  'drowned',\n",
              "  'demolished',\n",
              "  'cool',\n",
              "  'both',\n",
              "  'bioterror',\n",
              "  'believe',\n",
              "  'avalanche',\n",
              "  'arson',\n",
              "  'turkey',\n",
              "  'snowstorm',\n",
              "  'site',\n",
              "  'shot',\n",
              "  'shooting',\n",
              "  'pic',\n",
              "  'nowplaying',\n",
              "  'media',\n",
              "  'islam',\n",
              "  'inside',\n",
              "  'hijack',\n",
              "  'helicopter',\n",
              "  'fight',\n",
              "  'fatality',\n",
              "  'fan',\n",
              "  'electrocute',\n",
              "  'doesnt',\n",
              "  'building',\n",
              "  'brown',\n",
              "  'bc',\n",
              "  'actually',\n",
              "  '16yr',\n",
              "  'yes',\n",
              "  'watching',\n",
              "  'wait',\n",
              "  'ur',\n",
              "  'tell',\n",
              "  'swallowed',\n",
              "  'seismic',\n",
              "  'second',\n",
              "  'rubble',\n",
              "  're\\x89Û',\n",
              "  'plans',\n",
              "  'men',\n",
              "  'memories',\n",
              "  'line',\n",
              "  'la',\n",
              "  'horror',\n",
              "  'health',\n",
              "  'having',\n",
              "  'find',\n",
              "  'eyewitness',\n",
              "  'deluged',\n",
              "  'children',\n",
              "  'bush',\n",
              "  'anything',\n",
              "  'already',\n",
              "  'almost',\n",
              "  'aircraft',\n",
              "  'yourself',\n",
              "  'yeah',\n",
              "  'whats',\n",
              "  'tomorrow',\n",
              "  'such',\n",
              "  'start',\n",
              "  'side',\n",
              "  'searching',\n",
              "  'saved',\n",
              "  'reactor',\n",
              "  'probably',\n",
              "  'play',\n",
              "  'person',\n",
              "  'peace',\n",
              "  'outside',\n",
              "  'officer',\n",
              "  'nearby',\n",
              "  'n',\n",
              "  'maybe',\n",
              "  'lost',\n",
              "  'literally',\n",
              "  'hours',\n",
              "  'hear',\n",
              "  'far',\n",
              "  'die',\n",
              "  'demolition',\n",
              "  'data',\n",
              "  'crews',\n",
              "  'conclusively',\n",
              "  'business',\n",
              "  'american',\n",
              "  '20',\n",
              "  '\\x89ÛÓ',\n",
              "  'west',\n",
              "  'waves',\n",
              "  'team',\n",
              "  'street',\n",
              "  'stay',\n",
              "  'soudelor',\n",
              "  'reuters',\n",
              "  'manslaughter',\n",
              "  'leather',\n",
              "  'job',\n",
              "  'history',\n",
              "  'hey',\n",
              "  'feeling',\n",
              "  'eyes',\n",
              "  'everything',\n",
              "  'declares',\n",
              "  'deal',\n",
              "  'casualty',\n",
              "  'bodies',\n",
              "  'amid',\n",
              "  'ablaze',\n",
              "  '7',\n",
              "  '50',\n",
              "  '30',\n",
              "  '12',\n",
              "  'youth',\n",
              "  'wont',\n",
              "  'wake',\n",
              "  'theyre',\n",
              "  'support',\n",
              "  'stretcher',\n",
              "  'same',\n",
              "  'rise',\n",
              "  'picking',\n",
              "  'photos',\n",
              "  'own',\n",
              "  'others',\n",
              "  'order',\n",
              "  'omg',\n",
              "  'okay',\n",
              "  'name',\n",
              "  'myself',\n",
              "  'money',\n",
              "  'makes',\n",
              "  'leave',\n",
              "  'lab',\n",
              "  'gt',\n",
              "  'gets',\n",
              "  'flag',\n",
              "  'desolate',\n",
              "  'crisis',\n",
              "  'center',\n",
              "  'book',\n",
              "  'blight',\n",
              "  'blaze',\n",
              "  'ago',\n",
              "  'abc',\n",
              "  '11yearold',\n",
              "  'womens',\n",
              "  'typhoondevastated',\n",
              "  'tv',\n",
              "  'trench',\n",
              "  'trains',\n",
              "  'texas',\n",
              "  'space',\n",
              "  'siren',\n",
              "  'shes',\n",
              "  'self',\n",
              "  'saipan',\n",
              "  'reason',\n",
              "  'rd',\n",
              "  'pretty',\n",
              "  'pick',\n",
              "  'offensive',\n",
              "  'move',\n",
              "  'meek',\n",
              "  'major',\n",
              "  'm',\n",
              "  'low',\n",
              "  'lord',\n",
              "  'huge',\n",
              "  'hat',\n",
              "  'flash',\n",
              "  'feared',\n",
              "  'fast',\n",
              "  'effect',\n",
              "  'course',\n",
              "  'country',\n",
              "  'control',\n",
              "  'class',\n",
              "  'child',\n",
              "  'chance',\n",
              "  'caught',\n",
              "  'called',\n",
              "  'bioterrorism',\n",
              "  'bestnaijamade',\n",
              "  'become',\n",
              "  'bar',\n",
              "  'banned',\n",
              "  'ball',\n",
              "  'aug',\n",
              "  'annihilation',\n",
              "  'wrong',\n",
              "  'win',\n",
              "  'usa',\n",
              "  'united',\n",
              "  'town',\n",
              "  'totally',\n",
              "  'toddler',\n",
              "  'though',\n",
              "  'temple',\n",
              "  'taken',\n",
              "  'stand',\n",
              "  'spot',\n",
              "  'signs',\n",
              "  'ship',\n",
              "  'pakistan',\n",
              "  'online',\n",
              "  'level',\n",
              "  'ladies',\n",
              "  'jobs',\n",
              "  'isnt',\n",
              "  'happy',\n",
              "  'hailstorm',\n",
              "  'friends',\n",
              "  'disea',\n",
              "  'damn',\n",
              "  'couple',\n",
              "  'case',\n",
              "  'blue',\n",
              "  'bigger',\n",
              "  'america',\n",
              "  'across',\n",
              "  '10',\n",
              "  'yours',\n",
              "  'village',\n",
              "  'try',\n",
              "  'transport',\n",
              "  'talk',\n",
              "  'seen',\n",
              "  'russian',\n",
              "  'radio',\n",
              "  'projected',\n",
              "  'once',\n",
              "  'official',\n",
              "  'needs',\n",
              "  'nearly',\n",
              "  'mount',\n",
              "  'might',\n",
              "  'mayhem',\n",
              "  'instead',\n",
              "  'hollywood',\n",
              "  'haha',\n",
              "  'guy',\n",
              "  'gun',\n",
              "  'green',\n",
              "  'front',\n",
              "  'finally',\n",
              "  'favorite',\n",
              "  'experts',\n",
              "  'entire',\n",
              "  'east',\n",
              "  'daily',\n",
              "  'crazy',\n",
              "  'computers',\n",
              "  'coaches',\n",
              "  'christian',\n",
              "  'china',\n",
              "  'blizzard',\n",
              "  'anyone',\n",
              "  'aint',\n",
              "  'action',\n",
              "  '25',\n",
              "  'virgin',\n",
              "  'vehicle',\n",
              "  'truth',\n",
              "  'trust',\n",
              "  'takes',\n",
              "  't',\n",
              "  'star',\n",
              "  'sorry',\n",
              "  'running',\n",
              "  'refugio',\n",
              "  'reddits',\n",
              "  'poor',\n",
              "  'pain',\n",
              "  'mom',\n",
              "  'miners',\n",
              "  'marks',\n",
              "  'looking',\n",
              "  'knock',\n",
              "  'issued',\n",
              "  'insurance',\n",
              "  'ignition',\n",
              "  'houses',\n",
              "  'heavy',\n",
              "  'hate',\n",
              "  'hard',\n",
              "  'happened',\n",
              "  'global',\n",
              "  'giant',\n",
              "  'gbbo',\n",
              "  'flight',\n",
              "  'eye',\n",
              "  'emmerdale',\n",
              "  'driver',\n",
              "  'devastated',\n",
              "  'd',\n",
              "  'costlier',\n",
              "  'cnn',\n",
              "  'cars',\n",
              "  'camp',\n",
              "  'beach',\n",
              "  'arsonist',\n",
              "  'angry',\n",
              "  'alone',\n",
              "  'added',\n",
              "  '05',\n",
              "  'york',\n",
              "  'wonder',\n",
              "  'uk',\n",
              "  'turn',\n",
              "  'taking',\n",
              "  'subreddits',\n",
              "  'sounds',\n",
              "  'scared',\n",
              "  'russia',\n",
              "  'rly',\n",
              "  'reports',\n",
              "  'ready',\n",
              "  'quiz',\n",
              "  'public',\n",
              "  'property',\n",
              "  'pradesh',\n",
              "  'ppl',\n",
              "  'playing',\n",
              "  'pay',\n",
              "  'parole',\n",
              "  'pamela',\n",
              "  'pakistani',\n",
              "  'outrage',\n",
              "  'niggas',\n",
              "  'nagasaki',\n",
              "  'myanmar',\n",
              "  'muslims',\n",
              "  'mop',\n",
              "  'madhya',\n",
              "  'mad',\n",
              "  'lmao',\n",
              "  'learn',\n",
              "  'large',\n",
              "  'govt',\n",
              "  'give',\n",
              "  'gems',\n",
              "  'gave',\n",
              "  'funtenna',\n",
              "  'fukushima',\n",
              "  'former',\n",
              "  'film',\n",
              "  'earth',\n",
              "  'drive',\n",
              "  'downtown',\n",
              "  'dog',\n",
              "  'comes',\n",
              "  'closed',\n",
              "  'cake',\n",
              "  'british',\n",
              "  'bring',\n",
              "  'bbc',\n",
              "  'b',\n",
              "  'appears',\n",
              "  'aftershock',\n",
              "  '13',\n",
              "  '11',\n",
              "  'young',\n",
              "  'wow',\n",
              "  'worst',\n",
              "  'waving',\n",
              "  'washington',\n",
              "  'wanted',\n",
              "  'vs',\n",
              "  'view',\n",
              "  'upon',\n",
              "  'tweet',\n",
              "  'tree',\n",
              "  'tote',\n",
              "  'thousands',\n",
              "  'thinking',\n",
              "  'theater',\n",
              "  'soul',\n",
              "  'sky',\n",
              "  'sign',\n",
              "  'shows',\n",
              "  'shift',\n",
              "  'seeing',\n",
              "  'sea',\n",
              "  'scene',\n",
              "  'safety',\n",
              "  'rules',\n",
              "  'rock',\n",
              "  'reported',\n",
              "  'r',\n",
              "  'pray',\n",
              "  'playlist',\n",
              "  'patience',\n",
              "  ...],\n",
              " ['', '[UNK]', 'the', 'a', 'in'],\n",
              " ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1'])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating an embedding using an embedding layer\n",
        "\n",
        "To make our embedding, we're going to use the TF's embedding layer.\n",
        "\n",
        "The parameters we care the most about our embedding layers are:\n",
        "* input_dim= sie of our vocab\n",
        "* output_dim= size of the output embedding vector. A value of a 100 will mean that each token will be represented as a vector of 100 long length\n",
        "* input_length= length of sequences being passed to the embedding layer\n",
        "*"
      ],
      "metadata": {
        "id": "UscIzRK9b6M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "05ZNG325d5QR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding=layers.Embedding(input_dim=max_vocab_length,\n",
        "                           output_dim=128,\n",
        "                           input_length=max_length)"
      ],
      "metadata": {
        "id": "sQrtz6UzEDid"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFptstOWEVJj",
        "outputId": "57d827c2-8a36-4fe7-d951-df82d99d7ffb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.core.embedding.Embedding at 0x7faab07d6ef0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get a random sentence from the training set\n",
        "random_sentence=random.choice(train_sentences)\n",
        "print(f\"Original Sentence:{random_sentence}\\\n",
        "      \\nEmbedded Sentence\")\n",
        "\n",
        "sample_embedding=embedding(text_vectorizer([random_sentence]))\n",
        "sample_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0XRCbb8EYo0",
        "outputId": "3ccd2b4c-af0b-48b1-90c4-c28708a73a88"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence:MaFireEMS: RT WMUR9: Two buildings involved in fire on 2nd Street in #Manchester. WMUR9  http://t.co/QUFwXRJIql via KCarosaWMUR      \n",
            "Embedded Sentence\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[ 0.01783091,  0.02592481,  0.0186563 , ...,  0.02584991,\n",
              "         -0.02341678,  0.02544465],\n",
              "        [ 0.02098122, -0.03860571,  0.03564078, ...,  0.02438277,\n",
              "          0.00278072, -0.02660488],\n",
              "        [-0.01250132,  0.04893612,  0.01884257, ..., -0.03141469,\n",
              "          0.03925247, -0.04230182],\n",
              "        ...,\n",
              "        [-0.03282436,  0.03768099, -0.02799851, ...,  0.00510786,\n",
              "         -0.0426378 ,  0.01679177],\n",
              "        [-0.01250132,  0.04893612,  0.01884257, ..., -0.03141469,\n",
              "          0.03925247, -0.04230182],\n",
              "        [ 0.01783091,  0.02592481,  0.0186563 , ...,  0.02584991,\n",
              "         -0.02341678,  0.02544465]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check out a single token's embedding\n",
        "sample_embedding[0][0], sample_embedding[0][0].shape, random_sentence[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12ur4eOWE9w1",
        "outputId": "7f5ecb78-1700-4c95-b95e-a0fd95e3300b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              " array([ 0.01783091,  0.02592481,  0.0186563 , -0.01002058, -0.03591248,\n",
              "        -0.04636083,  0.00280815, -0.02864938, -0.01817731, -0.03952376,\n",
              "        -0.00392552,  0.01689071,  0.02733884, -0.03937083,  0.0292857 ,\n",
              "         0.03110827,  0.03497079,  0.04608897,  0.00704961, -0.02222641,\n",
              "        -0.01527553,  0.03682159,  0.00765996, -0.01493281,  0.00495691,\n",
              "         0.02261562, -0.03590884, -0.04934757,  0.04630036,  0.02650198,\n",
              "        -0.01972774,  0.01520215,  0.03853578,  0.00094329,  0.04524038,\n",
              "         0.00766432,  0.04982356, -0.00425726,  0.03033056,  0.01132816,\n",
              "        -0.02919952, -0.02731879,  0.02447276,  0.00015778,  0.006719  ,\n",
              "         0.00910298, -0.00546825,  0.02265335, -0.02276393, -0.01733585,\n",
              "         0.03793552, -0.01228113,  0.01318524,  0.01964075,  0.03335306,\n",
              "        -0.03348223,  0.03906039, -0.03872584,  0.01063757,  0.00022243,\n",
              "        -0.04300516,  0.00541227,  0.04874512,  0.03780986, -0.00805902,\n",
              "        -0.03334194, -0.0234429 ,  0.03583051,  0.00835315, -0.02240558,\n",
              "         0.0361363 , -0.03155778,  0.00140536,  0.02706387, -0.01191444,\n",
              "         0.03514436, -0.02357372, -0.00665156, -0.00538865, -0.03504616,\n",
              "        -0.04433588, -0.03104684,  0.04267862, -0.04221052, -0.00430929,\n",
              "        -0.00995495,  0.02308554, -0.01211163,  0.03074156,  0.0104404 ,\n",
              "        -0.03717912, -0.04555782, -0.02618849,  0.04004676, -0.00310196,\n",
              "        -0.00470831,  0.02572955, -0.02296372, -0.04593062,  0.01790074,\n",
              "        -0.03977672, -0.04241864, -0.04021522,  0.02588839,  0.00960157,\n",
              "         0.01340936, -0.0153438 , -0.02752626, -0.01006345, -0.03000052,\n",
              "         0.04580015,  0.01842815,  0.00600166, -0.02792805,  0.02398106,\n",
              "         0.03946659,  0.03094896, -0.01033736, -0.01625506, -0.0408917 ,\n",
              "         0.01461701,  0.02919896,  0.01336152,  0.04583858, -0.0110124 ,\n",
              "         0.02584991, -0.02341678,  0.02544465], dtype=float32)>,\n",
              " TensorShape([128]),\n",
              " 'M')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modelling a text dataset(running a series of experiments)\n",
        "\n",
        "* Model 0: Naive Bayes with Tokenization\n",
        "* Model 1: Feed-forward Neural Network(Dense Model)\n",
        "* Model 2: LSTM Model(RNN)\n",
        "* Model 3: GRU Model\n",
        "* Moel 4: Bidirectional LSTM\n",
        "* Model 5 : 1D Convolutional Layer\n",
        "* model 6: Tensorflow hub pretrained feature extraction (using transfer learning)\n",
        "* Model 7 : Same as model 6 with 10% of data\n",
        "\n",
        "How are we going to approach all of these:\n",
        "Use the standard steps in modelling with tensorflow\n",
        "\n",
        "* Create->build->fit->Evaluate\n",
        "\n"
      ],
      "metadata": {
        "id": "FhiG7RYnF6Le"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aXUkwxQcI759"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}