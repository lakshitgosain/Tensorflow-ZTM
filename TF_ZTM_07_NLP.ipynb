{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMg4KihNaCHwdSHSLrrqzTI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshitgosain/Tensorflow-ZTM/blob/main/TF_ZTM_07_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Fundamentals in TF"
      ],
      "metadata": {
        "id": "CKP8rK-46aDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence-to-sequence\n",
        "* One to one- Image Captioning\n",
        "* Many to one- Sentiment analysis\n",
        "* Many to one- time series forecasting\n",
        "* many- to-many- Machine translation\n"
      ],
      "metadata": {
        "id": "UEIfyi-r8kcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfEm1Kes8lgu",
        "outputId": "b6c2dc88-6a35-451e-a54d-05b3c6d54cfc"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3FIIV0NRezo",
        "outputId": "fc206816-a7b9-4154-b13e-cfcf76971d42"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-16 03:12:50--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py.1’\n",
            "\n",
            "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-16 03:12:50 (66.8 MB/s) - ‘helper_functions.py.1’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
      ],
      "metadata": {
        "id": "FZdua_xCR3nj"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu1aEiChSBoa",
        "outputId": "ef6a4a3b-98b1-41ce-bc68-21c20bed86af"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-16 03:12:50--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.107.128, 74.125.196.128, 74.125.134.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.107.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp_getting_started.zip.1’\n",
            "\n",
            "nlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-06-16 03:12:51 (102 MB/s) - ‘nlp_getting_started.zip.1’ saved [607343/607343]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unzip_data(\"nlp_getting_started.zip\")"
      ],
      "metadata": {
        "id": "ZwhaaRJFSXGF"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the dataset\n",
        "\n",
        "To visualize the sample data, we need to read the in. On way is to use python.\n",
        "\n",
        "Another way to do this is use pandas\n",
        "\n"
      ],
      "metadata": {
        "id": "FnODZX-2SZtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df=pd.read_csv(\"train.csv\")\n",
        "test_df=pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "OA7Fu5nwKZJE"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "yECy1rYFLgUa",
        "outputId": "0eab2909-3132-40c3-bbe5-5689b806b3d7"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a50684a8-c985-45c6-8a50-b92d9091ec28\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a50684a8-c985-45c6-8a50-b92d9091ec28')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a50684a8-c985-45c6-8a50-b92d9091ec28 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a50684a8-c985-45c6-8a50-b92d9091ec28');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_shuffled=train_df.sample(frac=1, random_state=42)"
      ],
      "metadata": {
        "id": "6mcpKv3ULfok"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_shuffled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "rheeFHeLMEPe",
        "outputId": "796d4221-07e9-4f92-94c3-6044ada3b0e5"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id       keyword                        location  \\\n",
              "2644   3796   destruction                             NaN   \n",
              "2227   3185        deluge                             NaN   \n",
              "5448   7769        police                              UK   \n",
              "132     191    aftershock                             NaN   \n",
              "6845   9810        trauma           Montgomery County, MD   \n",
              "...     ...           ...                             ...   \n",
              "5226   7470  obliteration                         Merica!   \n",
              "5390   7691         panic                             NaN   \n",
              "860    1242         blood                             NaN   \n",
              "7603  10862           NaN                             NaN   \n",
              "7270  10409     whirlwind  Stamford & Cork (& Shropshire)   \n",
              "\n",
              "                                                   text  target  \n",
              "2644  So you have a new weapon that can cause un-ima...       1  \n",
              "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
              "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
              "132   Aftershock back to school kick off was great. ...       0  \n",
              "6845  in response to trauma Children of Addicts deve...       0  \n",
              "...                                                 ...     ...  \n",
              "5226  @Eganator2000 There aren't many Obliteration s...       0  \n",
              "5390  just had a panic attack bc I don't have enough...       0  \n",
              "860   Omron HEM-712C Automatic Blood Pressure Monito...       0  \n",
              "7603  Officials say a quarantine is in place at an A...       1  \n",
              "7270  I moved to England five years ago today. What ...       1  \n",
              "\n",
              "[7613 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95bbafc7-cbe2-4079-8c53-b39a1d47e8eb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>3796</td>\n",
              "      <td>destruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>So you have a new weapon that can cause un-ima...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>3185</td>\n",
              "      <td>deluge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>7769</td>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>191</td>\n",
              "      <td>aftershock</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Aftershock back to school kick off was great. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>9810</td>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts deve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5226</th>\n",
              "      <td>7470</td>\n",
              "      <td>obliteration</td>\n",
              "      <td>Merica!</td>\n",
              "      <td>@Eganator2000 There aren't many Obliteration s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>7691</td>\n",
              "      <td>panic</td>\n",
              "      <td>NaN</td>\n",
              "      <td>just had a panic attack bc I don't have enough...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>1242</td>\n",
              "      <td>blood</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Omron HEM-712C Automatic Blood Pressure Monito...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7603</th>\n",
              "      <td>10862</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Officials say a quarantine is in place at an A...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7270</th>\n",
              "      <td>10409</td>\n",
              "      <td>whirlwind</td>\n",
              "      <td>Stamford &amp; Cork (&amp; Shropshire)</td>\n",
              "      <td>I moved to England five years ago today. What ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7613 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95bbafc7-cbe2-4079-8c53-b39a1d47e8eb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95bbafc7-cbe2-4079-8c53-b39a1d47e8eb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95bbafc7-cbe2-4079-8c53-b39a1d47e8eb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test dataframe looks like:\n",
        "test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EOKnKXMgMLmz",
        "outputId": "33dcad65-a2e2-4bb4-94e3-07d285ab9b43"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9781ae02-2ed8-42f8-b470-1fabec2776e4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9781ae02-2ed8-42f8-b470-1fabec2776e4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9781ae02-2ed8-42f8-b470-1fabec2776e4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9781ae02-2ed8-42f8-b470-1fabec2776e4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVroyArwMSAd",
        "outputId": "6395cbd2-c27c-49a8-beb4-9d3d59fea2bf"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#How many total number of samples\n",
        "len(train_df), len(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJwjZWDvMbP0",
        "outputId": "352c578a-4953-40aa-b89b-f5b516e5537e"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7613, 3263)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's visualize some random training samples\n",
        "import random\n",
        "random_index = random.randint(0,len(train_df)-5)\n",
        "for row in train_data_shuffled[['text','target']][random_index:random_index+5].itertuples():\n",
        "  _,text, target=row\n",
        "  print(f\"Target: {target}\", \"(real disaster)\" if target >0 else \"(not real disaster)\")\n",
        "  print(f\"text:\\n{text}\\n\")\n",
        "  print(\"---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDoIh77fM8xb",
        "outputId": "98ae59ab-a953-496d-97b9-df9010c2fbb2"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target: 1 (real disaster)\n",
            "text:\n",
            "Wreckage 'Conclusively Confirmed' as From MH370: Malaysia PM: Investigators and the families of those who were... http://t.co/MSsq0sVnBM\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "text:\n",
            "Russian 'food crematoria' provoke outrage amid crisis famine memories - Yahoo News http://t.co/6siiRlnV6z\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "text:\n",
            "The ol' meltdown victory for the Mets.\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "text:\n",
            "More Natural Disaster Research Urgent http://t.co/5Cm0LfZhxn via #JakartaPost\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster)\n",
            "text:\n",
            "I have been bleeding into this typewriter all day but so far all I've written is a bunch of gunk.\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data into training and validation sets\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3_KYNkVORfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "K-_DDtcAeap3"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, val_sentences, train_labels, val_labels=train_test_split(train_data_shuffled['text'].to_numpy(),\n",
        "                                                                          train_data_shuffled['target'].to_numpy(),\n",
        "                                                                          test_size=0.1,\n",
        "                                                                          random_state=42)"
      ],
      "metadata": {
        "id": "rFRtXZeded8h"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the lengths\n",
        "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvV1nQX6fD8S",
        "outputId": "ca3ee263-9311-4451-fc9d-957977a8bb40"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6851, 6851, 762, 762)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting words into numbers\n",
        "\n",
        "First thing while building a model is to convert your text into numbers\n"
      ],
      "metadata": {
        "id": "xQJN3MYYktnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization VS Embedding\n",
        "Tkenization - word level and character level. Direct apping of a token.\n",
        "\n",
        "Embedding - every word gets turned into a vector and we can define size of the vector. Embeddings can learn as our model trains\n",
        "\n"
      ],
      "metadata": {
        "id": "6GOhJ-mcfRFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text vectorization(tokenization)"
      ],
      "metadata": {
        "id": "CZ_FBHbwjhjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "metadata": {
        "id": "otEJGiTzJmNX"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the default text vectorization parameters\n",
        "text_vectoizer= TextVectorization(max_tokens=None, #Defines how many words in our vocab\n",
        "                               standardize= \"lower_and_strip_punctuation\",\n",
        "                               split=\"whitespace\",\n",
        "                               ngrams=None,\n",
        "                               output_mode='int',\n",
        "                               output_sequence_length=None) #how long do you want your sequences to be\n",
        "                               #pad_to_max_tokens=True)"
      ],
      "metadata": {
        "id": "1iQnXkkeJvaQ"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[0].split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7P5w8FmXiRk",
        "outputId": "6689b01f-f8ef-4800-c051-91bbc32ef9a2"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@mogacola', '@zamtriossu', 'i', 'screamed', 'after', 'hitting', 'tweet']"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the average number of tokens(words) in the training tweets\n",
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUNdYmJ9WjZY",
        "outputId": "f6e33307-e604-4dc0-adc0-7976b24e6708"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_length= 10000 #max no of words to have in our vocab\n",
        "max_length=15 #Max length our sequences will be (e.g. how many words from a Tweet does a model see)\n",
        "\n",
        "text_vectorizer=TextVectorization(max_tokens=max_vocab_length,\n",
        "                                     output_mode=\"int\",\n",
        "                                     output_sequence_length=max_length)"
      ],
      "metadata": {
        "id": "LMx1h8VWX4uS"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the text vectorizer to the training text\n",
        "text_vectorizer.adapt(train_sentences)\n"
      ],
      "metadata": {
        "id": "yo-nO5JSYXiy"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIUXVbetZ-sI",
        "outputId": "5b33a77f-817a-47fa-ee0e-d94655f9351e"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "       'Imagine getting flattened by Kurt Zouma',\n",
              "       '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "       \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "       'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
              "       '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
              "       'destroy the free fandom honestly',\n",
              "       'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
              "       '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
              "       'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a sample sentence and tokenize it\n",
        "sample_sentence=\"There is a floow in my street!\"\n",
        "text_vectorizer([sample_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfFwEaKYaAoe",
        "outputId": "f23b6155-7bab-4084-83a6-e5fcd347de41"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[ 74,   9,   3,   1,   4,  13, 698,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_sentence=random.choice(train_sentences)\n",
        "print(random_sentence)\n",
        "vectorized_random_sentence=text_vectorizer([random_sentence])\n",
        "print(f\"Random Sentence: {random_sentence}\\nVectoized Random Sentence: {vectorized_random_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfpgC8tgaN4z",
        "outputId": "c278e6e3-7bd1-41be-8f6d-22ab0a5800e9"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FedEx no longer will ship potential bioterror pathogens - FedEx Corp. (NYSE: FDX) will no longer deliver packages ... http://t.co/2kdq56xTWs\n",
            "Random Sentence: FedEx no longer will ship potential bioterror pathogens - FedEx Corp. (NYSE: FDX) will no longer deliver packages ... http://t.co/2kdq56xTWs\n",
            "Vectoized Random Sentence: [[ 577   40  600   38  810 1615  609 1801  577 5912    1    1   38   40\n",
            "   600]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_vocab=text_vectorizer.get_vocabulary()\n",
        "top_5_words=words_in_vocab[:5]\n",
        "botton_5_words=words_in_vocab[-5:]"
      ],
      "metadata": {
        "id": "ijV2vvTqbNLm"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_vocab,top_5_words,botton_5_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WMshKSybv4a",
        "outputId": "313f64b7-0b68-4067-8b90-82cd360bba51"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['',\n",
              "  '[UNK]',\n",
              "  'the',\n",
              "  'a',\n",
              "  'in',\n",
              "  'to',\n",
              "  'of',\n",
              "  'and',\n",
              "  'i',\n",
              "  'is',\n",
              "  'for',\n",
              "  'on',\n",
              "  'you',\n",
              "  'my',\n",
              "  'with',\n",
              "  'it',\n",
              "  'that',\n",
              "  'at',\n",
              "  'by',\n",
              "  'this',\n",
              "  'from',\n",
              "  'be',\n",
              "  'are',\n",
              "  'was',\n",
              "  'have',\n",
              "  'like',\n",
              "  'as',\n",
              "  'up',\n",
              "  'so',\n",
              "  'just',\n",
              "  'but',\n",
              "  'me',\n",
              "  'im',\n",
              "  'your',\n",
              "  'not',\n",
              "  'amp',\n",
              "  'out',\n",
              "  'its',\n",
              "  'will',\n",
              "  'an',\n",
              "  'no',\n",
              "  'has',\n",
              "  'fire',\n",
              "  'after',\n",
              "  'all',\n",
              "  'when',\n",
              "  'we',\n",
              "  'if',\n",
              "  'now',\n",
              "  'via',\n",
              "  'new',\n",
              "  'more',\n",
              "  'get',\n",
              "  'or',\n",
              "  'about',\n",
              "  'what',\n",
              "  'he',\n",
              "  'people',\n",
              "  'news',\n",
              "  'been',\n",
              "  'over',\n",
              "  'one',\n",
              "  'how',\n",
              "  'dont',\n",
              "  'they',\n",
              "  'who',\n",
              "  'into',\n",
              "  'were',\n",
              "  'do',\n",
              "  'us',\n",
              "  '2',\n",
              "  'can',\n",
              "  'video',\n",
              "  'emergency',\n",
              "  'there',\n",
              "  'disaster',\n",
              "  'than',\n",
              "  'police',\n",
              "  'would',\n",
              "  'his',\n",
              "  'still',\n",
              "  'her',\n",
              "  'some',\n",
              "  'body',\n",
              "  'storm',\n",
              "  'crash',\n",
              "  'burning',\n",
              "  'suicide',\n",
              "  'back',\n",
              "  'man',\n",
              "  'california',\n",
              "  'why',\n",
              "  'time',\n",
              "  'them',\n",
              "  'had',\n",
              "  'buildings',\n",
              "  'rt',\n",
              "  'first',\n",
              "  'cant',\n",
              "  'see',\n",
              "  'got',\n",
              "  'day',\n",
              "  'off',\n",
              "  'our',\n",
              "  'going',\n",
              "  'nuclear',\n",
              "  'know',\n",
              "  'world',\n",
              "  'bomb',\n",
              "  'fires',\n",
              "  'love',\n",
              "  'killed',\n",
              "  'go',\n",
              "  'attack',\n",
              "  'youtube',\n",
              "  'dead',\n",
              "  'two',\n",
              "  'families',\n",
              "  '3',\n",
              "  'train',\n",
              "  'full',\n",
              "  'being',\n",
              "  'war',\n",
              "  'many',\n",
              "  'today',\n",
              "  'think',\n",
              "  'only',\n",
              "  'car',\n",
              "  'accident',\n",
              "  'life',\n",
              "  'hiroshima',\n",
              "  'their',\n",
              "  'say',\n",
              "  'may',\n",
              "  'down',\n",
              "  'watch',\n",
              "  'good',\n",
              "  'could',\n",
              "  'want',\n",
              "  'last',\n",
              "  'here',\n",
              "  'years',\n",
              "  'u',\n",
              "  'then',\n",
              "  'make',\n",
              "  'did',\n",
              "  'wildfire',\n",
              "  'way',\n",
              "  'help',\n",
              "  'best',\n",
              "  'too',\n",
              "  'even',\n",
              "  'because',\n",
              "  'home',\n",
              "  'death',\n",
              "  'collapse',\n",
              "  'bombing',\n",
              "  'mass',\n",
              "  'him',\n",
              "  'black',\n",
              "  'am',\n",
              "  'those',\n",
              "  'need',\n",
              "  'fatal',\n",
              "  'army',\n",
              "  'another',\n",
              "  'work',\n",
              "  'take',\n",
              "  'should',\n",
              "  'really',\n",
              "  'please',\n",
              "  'mh370',\n",
              "  'youre',\n",
              "  'look',\n",
              "  'lol',\n",
              "  'hot',\n",
              "  'pm',\n",
              "  'legionnaires',\n",
              "  '4',\n",
              "  'right',\n",
              "  '5',\n",
              "  'let',\n",
              "  'city',\n",
              "  'year',\n",
              "  'wreck',\n",
              "  'school',\n",
              "  'northern',\n",
              "  'much',\n",
              "  'forest',\n",
              "  'bomber',\n",
              "  'water',\n",
              "  'she',\n",
              "  'never',\n",
              "  'read',\n",
              "  'latest',\n",
              "  'homes',\n",
              "  'great',\n",
              "  'every',\n",
              "  '1',\n",
              "  'live',\n",
              "  'god',\n",
              "  'fear',\n",
              "  'any',\n",
              "  '\\x89Û',\n",
              "  'under',\n",
              "  'said',\n",
              "  'old',\n",
              "  'floods',\n",
              "  '2015',\n",
              "  'getting',\n",
              "  'atomic',\n",
              "  'while',\n",
              "  'top',\n",
              "  'obama',\n",
              "  'feel',\n",
              "  'thats',\n",
              "  'since',\n",
              "  'near',\n",
              "  'flames',\n",
              "  'ever',\n",
              "  'come',\n",
              "  'where',\n",
              "  'these',\n",
              "  'military',\n",
              "  'japan',\n",
              "  'found',\n",
              "  'content',\n",
              "  'ass',\n",
              "  'without',\n",
              "  'weather',\n",
              "  'most',\n",
              "  'flooding',\n",
              "  'flood',\n",
              "  'damage',\n",
              "  'which',\n",
              "  'shit',\n",
              "  's',\n",
              "  'hope',\n",
              "  'everyone',\n",
              "  'before',\n",
              "  'stop',\n",
              "  'plan',\n",
              "  'malaysia',\n",
              "  'injured',\n",
              "  'hit',\n",
              "  'evacuation',\n",
              "  'during',\n",
              "  'debris',\n",
              "  'cross',\n",
              "  'coming',\n",
              "  'wild',\n",
              "  'well',\n",
              "  'times',\n",
              "  'sinking',\n",
              "  'oil',\n",
              "  'fucking',\n",
              "  'check',\n",
              "  'cause',\n",
              "  'weapons',\n",
              "  'truck',\n",
              "  'food',\n",
              "  'bloody',\n",
              "  'always',\n",
              "  'weapon',\n",
              "  'theres',\n",
              "  'state',\n",
              "  'little',\n",
              "  'injuries',\n",
              "  'free',\n",
              "  'wounded',\n",
              "  'summer',\n",
              "  'smoke',\n",
              "  'severe',\n",
              "  'reddit',\n",
              "  'next',\n",
              "  'movie',\n",
              "  'ive',\n",
              "  'hes',\n",
              "  'fall',\n",
              "  'evacuate',\n",
              "  'confirmed',\n",
              "  'bad',\n",
              "  'again',\n",
              "  'thunderstorm',\n",
              "  'set',\n",
              "  'night',\n",
              "  'natural',\n",
              "  'looks',\n",
              "  'heat',\n",
              "  'face',\n",
              "  'earthquake',\n",
              "  'boy',\n",
              "  'whole',\n",
              "  'until',\n",
              "  'thunder',\n",
              "  'through',\n",
              "  'says',\n",
              "  'panic',\n",
              "  'outbreak',\n",
              "  'made',\n",
              "  'lightning',\n",
              "  'fatalities',\n",
              "  'family',\n",
              "  'explosion',\n",
              "  'end',\n",
              "  'destroy',\n",
              "  'derailment',\n",
              "  'air',\n",
              "  'w',\n",
              "  'terrorist',\n",
              "  'survive',\n",
              "  'screaming',\n",
              "  'saudi',\n",
              "  'refugees',\n",
              "  'rain',\n",
              "  'murder',\n",
              "  'loud',\n",
              "  'liked',\n",
              "  'house',\n",
              "  'gonna',\n",
              "  'failure',\n",
              "  'collided',\n",
              "  'bag',\n",
              "  'attacked',\n",
              "  'ambulance',\n",
              "  '70',\n",
              "  'wind',\n",
              "  'services',\n",
              "  'save',\n",
              "  'report',\n",
              "  'migrants',\n",
              "  'head',\n",
              "  'explode',\n",
              "  'charged',\n",
              "  'change',\n",
              "  'big',\n",
              "  'also',\n",
              "  'wrecked',\n",
              "  'warning',\n",
              "  'update',\n",
              "  'run',\n",
              "  'rescuers',\n",
              "  'released',\n",
              "  'photo',\n",
              "  'massacre',\n",
              "  'injury',\n",
              "  'hurricane',\n",
              "  'high',\n",
              "  'hail',\n",
              "  'fuck',\n",
              "  'does',\n",
              "  'destroyed',\n",
              "  'bus',\n",
              "  'blood',\n",
              "  '40',\n",
              "  '\\x89ÛÒ',\n",
              "  'wreckage',\n",
              "  'violent',\n",
              "  'twister',\n",
              "  'trauma',\n",
              "  'tragedy',\n",
              "  'terrorism',\n",
              "  'survivors',\n",
              "  'survived',\n",
              "  'sinkhole',\n",
              "  'sandstorm',\n",
              "  'road',\n",
              "  'rioting',\n",
              "  'red',\n",
              "  'real',\n",
              "  'put',\n",
              "  'post',\n",
              "  'national',\n",
              "  'missing',\n",
              "  'landslide',\n",
              "  'keep',\n",
              "  'girl',\n",
              "  'drought',\n",
              "  'curfew',\n",
              "  'breaking',\n",
              "  'bags',\n",
              "  'white',\n",
              "  'twitter',\n",
              "  'tonight',\n",
              "  'structural',\n",
              "  'spill',\n",
              "  'service',\n",
              "  'screamed',\n",
              "  'rescued',\n",
              "  'rescue',\n",
              "  'phone',\n",
              "  'ok',\n",
              "  'oh',\n",
              "  'mosque',\n",
              "  'lives',\n",
              "  'horrible',\n",
              "  'harm',\n",
              "  'game',\n",
              "  'dust',\n",
              "  'destruction',\n",
              "  'deluge',\n",
              "  'deaths',\n",
              "  'crashed',\n",
              "  'cliff',\n",
              "  'catastrophe',\n",
              "  'boat',\n",
              "  'away',\n",
              "  'august',\n",
              "  'area',\n",
              "  'apocalypse',\n",
              "  'woman',\n",
              "  'whirlwind',\n",
              "  'traumatised',\n",
              "  'stock',\n",
              "  'saw',\n",
              "  'ruin',\n",
              "  'riot',\n",
              "  'quarantine',\n",
              "  'kills',\n",
              "  'island',\n",
              "  'investigators',\n",
              "  'ill',\n",
              "  'hostages',\n",
              "  'hazard',\n",
              "  'danger',\n",
              "  'call',\n",
              "  '15',\n",
              "  'women',\n",
              "  'windstorm',\n",
              "  'things',\n",
              "  'suspect',\n",
              "  'show',\n",
              "  'reunion',\n",
              "  'quarantined',\n",
              "  'lava',\n",
              "  'heart',\n",
              "  'engulfed',\n",
              "  'detonate',\n",
              "  'crush',\n",
              "  'collapsed',\n",
              "  'came',\n",
              "  'better',\n",
              "  'battle',\n",
              "  'armageddon',\n",
              "  'airplane',\n",
              "  'against',\n",
              "  'affected',\n",
              "  'use',\n",
              "  'trapped',\n",
              "  'thank',\n",
              "  'sunk',\n",
              "  'story',\n",
              "  'send',\n",
              "  'part',\n",
              "  'other',\n",
              "  'must',\n",
              "  'mudslide',\n",
              "  'market',\n",
              "  'iran',\n",
              "  'famine',\n",
              "  'exploded',\n",
              "  'electrocuted',\n",
              "  'ebay',\n",
              "  'displaced',\n",
              "  'derailed',\n",
              "  'derail',\n",
              "  'burned',\n",
              "  'bombed',\n",
              "  'blown',\n",
              "  'baby',\n",
              "  'around',\n",
              "  'zone',\n",
              "  'wave',\n",
              "  'wanna',\n",
              "  'sure',\n",
              "  'someone',\n",
              "  'screams',\n",
              "  'razed',\n",
              "  'power',\n",
              "  'obliterated',\n",
              "  'long',\n",
              "  'land',\n",
              "  'hundreds',\n",
              "  'heard',\n",
              "  'group',\n",
              "  'flattened',\n",
              "  'drown',\n",
              "  'doing',\n",
              "  'care',\n",
              "  'bridge',\n",
              "  'bagging',\n",
              "  '9',\n",
              "  'went',\n",
              "  'used',\n",
              "  'typhoon',\n",
              "  'trouble',\n",
              "  'tornado',\n",
              "  'thought',\n",
              "  'thing',\n",
              "  'river',\n",
              "  'responders',\n",
              "  'past',\n",
              "  'pandemonium',\n",
              "  'officials',\n",
              "  'meltdown',\n",
              "  'lot',\n",
              "  'least',\n",
              "  'inundated',\n",
              "  'id',\n",
              "  'hostage',\n",
              "  'hijacking',\n",
              "  'hazardous',\n",
              "  'goes',\n",
              "  'drowning',\n",
              "  'didnt',\n",
              "  'devastation',\n",
              "  'demolish',\n",
              "  'collide',\n",
              "  'casualties',\n",
              "  'calgary',\n",
              "  'bang',\n",
              "  'anniversary',\n",
              "  'yet',\n",
              "  'wounds',\n",
              "  'volcano',\n",
              "  'tsunami',\n",
              "  'sue',\n",
              "  'st',\n",
              "  'song',\n",
              "  'something',\n",
              "  'shoulder',\n",
              "  'security',\n",
              "  'prebreak',\n",
              "  'possible',\n",
              "  'pkk',\n",
              "  'panicking',\n",
              "  'obliteration',\n",
              "  'obliterate',\n",
              "  'murderer',\n",
              "  'minute',\n",
              "  'light',\n",
              "  'lets',\n",
              "  'kill',\n",
              "  'isis',\n",
              "  'india',\n",
              "  'hijacker',\n",
              "  'hellfire',\n",
              "  'government',\n",
              "  'few',\n",
              "  'evacuated',\n",
              "  'due',\n",
              "  'detonated',\n",
              "  'desolation',\n",
              "  'crushed',\n",
              "  'chemical',\n",
              "  'blew',\n",
              "  'blazing',\n",
              "  'blast',\n",
              "  'annihilated',\n",
              "  'airport',\n",
              "  '6',\n",
              "  'week',\n",
              "  'upheaval',\n",
              "  'trying',\n",
              "  'three',\n",
              "  'thanks',\n",
              "  'sound',\n",
              "  'soon',\n",
              "  'sirens',\n",
              "  'rainstorm',\n",
              "  'plane',\n",
              "  'music',\n",
              "  'making',\n",
              "  'kids',\n",
              "  'issues',\n",
              "  'half',\n",
              "  'guys',\n",
              "  'fedex',\n",
              "  'done',\n",
              "  'died',\n",
              "  'detonation',\n",
              "  'days',\n",
              "  'cyclone',\n",
              "  'county',\n",
              "  'collision',\n",
              "  'caused',\n",
              "  'catastrophic',\n",
              "  'bleeding',\n",
              "  'beautiful',\n",
              "  '8',\n",
              "  'words',\n",
              "  'very',\n",
              "  'traffic',\n",
              "  'south',\n",
              "  'remember',\n",
              "  'policy',\n",
              "  'place',\n",
              "  'nothing',\n",
              "  'north',\n",
              "  'mp',\n",
              "  'longer',\n",
              "  'left',\n",
              "  'israeli',\n",
              "  'hell',\n",
              "  'fun',\n",
              "  'drowned',\n",
              "  'demolished',\n",
              "  'cool',\n",
              "  'both',\n",
              "  'bioterror',\n",
              "  'believe',\n",
              "  'avalanche',\n",
              "  'arson',\n",
              "  'turkey',\n",
              "  'snowstorm',\n",
              "  'site',\n",
              "  'shot',\n",
              "  'shooting',\n",
              "  'pic',\n",
              "  'nowplaying',\n",
              "  'media',\n",
              "  'islam',\n",
              "  'inside',\n",
              "  'hijack',\n",
              "  'helicopter',\n",
              "  'fight',\n",
              "  'fatality',\n",
              "  'fan',\n",
              "  'electrocute',\n",
              "  'doesnt',\n",
              "  'building',\n",
              "  'brown',\n",
              "  'bc',\n",
              "  'actually',\n",
              "  '16yr',\n",
              "  'yes',\n",
              "  'watching',\n",
              "  'wait',\n",
              "  'ur',\n",
              "  'tell',\n",
              "  'swallowed',\n",
              "  'seismic',\n",
              "  'second',\n",
              "  'rubble',\n",
              "  're\\x89Û',\n",
              "  'plans',\n",
              "  'men',\n",
              "  'memories',\n",
              "  'line',\n",
              "  'la',\n",
              "  'horror',\n",
              "  'health',\n",
              "  'having',\n",
              "  'find',\n",
              "  'eyewitness',\n",
              "  'deluged',\n",
              "  'children',\n",
              "  'bush',\n",
              "  'anything',\n",
              "  'already',\n",
              "  'almost',\n",
              "  'aircraft',\n",
              "  'yourself',\n",
              "  'yeah',\n",
              "  'whats',\n",
              "  'tomorrow',\n",
              "  'such',\n",
              "  'start',\n",
              "  'side',\n",
              "  'searching',\n",
              "  'saved',\n",
              "  'reactor',\n",
              "  'probably',\n",
              "  'play',\n",
              "  'person',\n",
              "  'peace',\n",
              "  'outside',\n",
              "  'officer',\n",
              "  'nearby',\n",
              "  'n',\n",
              "  'maybe',\n",
              "  'lost',\n",
              "  'literally',\n",
              "  'hours',\n",
              "  'hear',\n",
              "  'far',\n",
              "  'die',\n",
              "  'demolition',\n",
              "  'data',\n",
              "  'crews',\n",
              "  'conclusively',\n",
              "  'business',\n",
              "  'american',\n",
              "  '20',\n",
              "  '\\x89ÛÓ',\n",
              "  'west',\n",
              "  'waves',\n",
              "  'team',\n",
              "  'street',\n",
              "  'stay',\n",
              "  'soudelor',\n",
              "  'reuters',\n",
              "  'manslaughter',\n",
              "  'leather',\n",
              "  'job',\n",
              "  'history',\n",
              "  'hey',\n",
              "  'feeling',\n",
              "  'eyes',\n",
              "  'everything',\n",
              "  'declares',\n",
              "  'deal',\n",
              "  'casualty',\n",
              "  'bodies',\n",
              "  'amid',\n",
              "  'ablaze',\n",
              "  '7',\n",
              "  '50',\n",
              "  '30',\n",
              "  '12',\n",
              "  'youth',\n",
              "  'wont',\n",
              "  'wake',\n",
              "  'theyre',\n",
              "  'support',\n",
              "  'stretcher',\n",
              "  'same',\n",
              "  'rise',\n",
              "  'picking',\n",
              "  'photos',\n",
              "  'own',\n",
              "  'others',\n",
              "  'order',\n",
              "  'omg',\n",
              "  'okay',\n",
              "  'name',\n",
              "  'myself',\n",
              "  'money',\n",
              "  'makes',\n",
              "  'leave',\n",
              "  'lab',\n",
              "  'gt',\n",
              "  'gets',\n",
              "  'flag',\n",
              "  'desolate',\n",
              "  'crisis',\n",
              "  'center',\n",
              "  'book',\n",
              "  'blight',\n",
              "  'blaze',\n",
              "  'ago',\n",
              "  'abc',\n",
              "  '11yearold',\n",
              "  'womens',\n",
              "  'typhoondevastated',\n",
              "  'tv',\n",
              "  'trench',\n",
              "  'trains',\n",
              "  'texas',\n",
              "  'space',\n",
              "  'siren',\n",
              "  'shes',\n",
              "  'self',\n",
              "  'saipan',\n",
              "  'reason',\n",
              "  'rd',\n",
              "  'pretty',\n",
              "  'pick',\n",
              "  'offensive',\n",
              "  'move',\n",
              "  'meek',\n",
              "  'major',\n",
              "  'm',\n",
              "  'low',\n",
              "  'lord',\n",
              "  'huge',\n",
              "  'hat',\n",
              "  'flash',\n",
              "  'feared',\n",
              "  'fast',\n",
              "  'effect',\n",
              "  'course',\n",
              "  'country',\n",
              "  'control',\n",
              "  'class',\n",
              "  'child',\n",
              "  'chance',\n",
              "  'caught',\n",
              "  'called',\n",
              "  'bioterrorism',\n",
              "  'bestnaijamade',\n",
              "  'become',\n",
              "  'bar',\n",
              "  'banned',\n",
              "  'ball',\n",
              "  'aug',\n",
              "  'annihilation',\n",
              "  'wrong',\n",
              "  'win',\n",
              "  'usa',\n",
              "  'united',\n",
              "  'town',\n",
              "  'totally',\n",
              "  'toddler',\n",
              "  'though',\n",
              "  'temple',\n",
              "  'taken',\n",
              "  'stand',\n",
              "  'spot',\n",
              "  'signs',\n",
              "  'ship',\n",
              "  'pakistan',\n",
              "  'online',\n",
              "  'level',\n",
              "  'ladies',\n",
              "  'jobs',\n",
              "  'isnt',\n",
              "  'happy',\n",
              "  'hailstorm',\n",
              "  'friends',\n",
              "  'disea',\n",
              "  'damn',\n",
              "  'couple',\n",
              "  'case',\n",
              "  'blue',\n",
              "  'bigger',\n",
              "  'america',\n",
              "  'across',\n",
              "  '10',\n",
              "  'yours',\n",
              "  'village',\n",
              "  'try',\n",
              "  'transport',\n",
              "  'talk',\n",
              "  'seen',\n",
              "  'russian',\n",
              "  'radio',\n",
              "  'projected',\n",
              "  'once',\n",
              "  'official',\n",
              "  'needs',\n",
              "  'nearly',\n",
              "  'mount',\n",
              "  'might',\n",
              "  'mayhem',\n",
              "  'instead',\n",
              "  'hollywood',\n",
              "  'haha',\n",
              "  'guy',\n",
              "  'gun',\n",
              "  'green',\n",
              "  'front',\n",
              "  'finally',\n",
              "  'favorite',\n",
              "  'experts',\n",
              "  'entire',\n",
              "  'east',\n",
              "  'daily',\n",
              "  'crazy',\n",
              "  'computers',\n",
              "  'coaches',\n",
              "  'christian',\n",
              "  'china',\n",
              "  'blizzard',\n",
              "  'anyone',\n",
              "  'aint',\n",
              "  'action',\n",
              "  '25',\n",
              "  'virgin',\n",
              "  'vehicle',\n",
              "  'truth',\n",
              "  'trust',\n",
              "  'takes',\n",
              "  't',\n",
              "  'star',\n",
              "  'sorry',\n",
              "  'running',\n",
              "  'refugio',\n",
              "  'reddits',\n",
              "  'poor',\n",
              "  'pain',\n",
              "  'mom',\n",
              "  'miners',\n",
              "  'marks',\n",
              "  'looking',\n",
              "  'knock',\n",
              "  'issued',\n",
              "  'insurance',\n",
              "  'ignition',\n",
              "  'houses',\n",
              "  'heavy',\n",
              "  'hate',\n",
              "  'hard',\n",
              "  'happened',\n",
              "  'global',\n",
              "  'giant',\n",
              "  'gbbo',\n",
              "  'flight',\n",
              "  'eye',\n",
              "  'emmerdale',\n",
              "  'driver',\n",
              "  'devastated',\n",
              "  'd',\n",
              "  'costlier',\n",
              "  'cnn',\n",
              "  'cars',\n",
              "  'camp',\n",
              "  'beach',\n",
              "  'arsonist',\n",
              "  'angry',\n",
              "  'alone',\n",
              "  'added',\n",
              "  '05',\n",
              "  'york',\n",
              "  'wonder',\n",
              "  'uk',\n",
              "  'turn',\n",
              "  'taking',\n",
              "  'subreddits',\n",
              "  'sounds',\n",
              "  'scared',\n",
              "  'russia',\n",
              "  'rly',\n",
              "  'reports',\n",
              "  'ready',\n",
              "  'quiz',\n",
              "  'public',\n",
              "  'property',\n",
              "  'pradesh',\n",
              "  'ppl',\n",
              "  'playing',\n",
              "  'pay',\n",
              "  'parole',\n",
              "  'pamela',\n",
              "  'pakistani',\n",
              "  'outrage',\n",
              "  'niggas',\n",
              "  'nagasaki',\n",
              "  'myanmar',\n",
              "  'muslims',\n",
              "  'mop',\n",
              "  'madhya',\n",
              "  'mad',\n",
              "  'lmao',\n",
              "  'learn',\n",
              "  'large',\n",
              "  'govt',\n",
              "  'give',\n",
              "  'gems',\n",
              "  'gave',\n",
              "  'funtenna',\n",
              "  'fukushima',\n",
              "  'former',\n",
              "  'film',\n",
              "  'earth',\n",
              "  'drive',\n",
              "  'downtown',\n",
              "  'dog',\n",
              "  'comes',\n",
              "  'closed',\n",
              "  'cake',\n",
              "  'british',\n",
              "  'bring',\n",
              "  'bbc',\n",
              "  'b',\n",
              "  'appears',\n",
              "  'aftershock',\n",
              "  '13',\n",
              "  '11',\n",
              "  'young',\n",
              "  'wow',\n",
              "  'worst',\n",
              "  'waving',\n",
              "  'washington',\n",
              "  'wanted',\n",
              "  'vs',\n",
              "  'view',\n",
              "  'upon',\n",
              "  'tweet',\n",
              "  'tree',\n",
              "  'tote',\n",
              "  'thousands',\n",
              "  'thinking',\n",
              "  'theater',\n",
              "  'soul',\n",
              "  'sky',\n",
              "  'sign',\n",
              "  'shows',\n",
              "  'shift',\n",
              "  'seeing',\n",
              "  'sea',\n",
              "  'scene',\n",
              "  'safety',\n",
              "  'rules',\n",
              "  'rock',\n",
              "  'reported',\n",
              "  'r',\n",
              "  'pray',\n",
              "  'playlist',\n",
              "  'patience',\n",
              "  ...],\n",
              " ['', '[UNK]', 'the', 'a', 'in'],\n",
              " ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1'])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating an embedding using an embedding layer\n",
        "\n",
        "To make our embedding, we're going to use the TF's embedding layer.\n",
        "\n",
        "The parameters we care the most about our embedding layers are:\n",
        "* input_dim= sie of our vocab\n",
        "* output_dim= size of the output embedding vector. A value of a 100 will mean that each token will be represented as a vector of 100 long length\n",
        "* input_length= length of sequences being passed to the embedding layer\n",
        "*"
      ],
      "metadata": {
        "id": "UscIzRK9b6M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "05ZNG325d5QR"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding=layers.Embedding(input_dim=max_vocab_length,\n",
        "                           output_dim=128,\n",
        "                           input_length=max_length)"
      ],
      "metadata": {
        "id": "sQrtz6UzEDid"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFptstOWEVJj",
        "outputId": "e439025b-2722-4132-ce98-8cbfd12714bb"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.core.embedding.Embedding at 0x7f204e61c1f0>"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get a random sentence from the training set\n",
        "random_sentence=random.choice(train_sentences)\n",
        "print(f\"Original Sentence:{random_sentence}\\\n",
        "      \\nEmbedded Sentence\")\n",
        "\n",
        "sample_embedding=embedding(text_vectorizer([random_sentence]))\n",
        "sample_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0XRCbb8EYo0",
        "outputId": "a24d690e-5a69-4288-debf-89ef0662bd50"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence:Ancient Mayan Tablet Found in Jungle Temple http://t.co/qp6q8RS8ON      \n",
            "Embedded Sentence\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[-0.03305274,  0.03334713, -0.0422186 , ..., -0.0002015 ,\n",
              "         -0.02461776, -0.03663777],\n",
              "        [-0.03749834,  0.04993585,  0.0281971 , ..., -0.01925508,\n",
              "          0.00739387, -0.04485953],\n",
              "        [ 0.02407706, -0.0220372 , -0.00616293, ..., -0.00148871,\n",
              "          0.00904542,  0.00793977],\n",
              "        ...,\n",
              "        [ 0.01678229, -0.04126339,  0.04923167, ..., -0.01411662,\n",
              "         -0.02210027, -0.00361538],\n",
              "        [ 0.01678229, -0.04126339,  0.04923167, ..., -0.01411662,\n",
              "         -0.02210027, -0.00361538],\n",
              "        [ 0.01678229, -0.04126339,  0.04923167, ..., -0.01411662,\n",
              "         -0.02210027, -0.00361538]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check out a single token's embedding\n",
        "sample_embedding[0][0], sample_embedding[0][0].shape, random_sentence[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12ur4eOWE9w1",
        "outputId": "70e01c14-f8bc-4b4f-982c-90acb72d732d"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              " array([-0.03305274,  0.03334713, -0.0422186 , -0.01651434, -0.01004869,\n",
              "         0.00807239,  0.02143629,  0.04050611, -0.0488528 ,  0.04384295,\n",
              "        -0.04994569, -0.02975096, -0.04748983,  0.03848847,  0.014861  ,\n",
              "        -0.03664464,  0.02235797, -0.01168202, -0.00961262, -0.02008259,\n",
              "         0.04554153,  0.04856726,  0.000729  ,  0.015517  ,  0.02172233,\n",
              "         0.01956419, -0.00673329, -0.01454289, -0.02853456, -0.01421569,\n",
              "        -0.03547049, -0.04265514,  0.04614327, -0.02024713,  0.0492197 ,\n",
              "         0.01474769, -0.03769908, -0.04338888,  0.00069172,  0.03630065,\n",
              "         0.01816435,  0.01300832, -0.01224078,  0.04164369, -0.03761123,\n",
              "        -0.03906231, -0.02251348,  0.01014551, -0.01259303,  0.01133149,\n",
              "         0.02600336,  0.02457358, -0.03501425,  0.04379206,  0.04179703,\n",
              "        -0.01283264,  0.02588114, -0.03014401, -0.0332365 , -0.00388657,\n",
              "         0.03525729,  0.01430574, -0.03992382,  0.01723776,  0.0177853 ,\n",
              "         0.0351144 , -0.00222255, -0.00097043, -0.03175728, -0.04309038,\n",
              "        -0.00272411, -0.03843832,  0.01778043, -0.03887849,  0.04332442,\n",
              "         0.0238304 ,  0.03619076,  0.02008159, -0.00306481,  0.01373894,\n",
              "         0.03334594, -0.04497937, -0.04185056, -0.0051312 ,  0.03231495,\n",
              "        -0.0305539 ,  0.02324258,  0.00124577,  0.01512993, -0.00121844,\n",
              "        -0.04031797, -0.03849963,  0.01569856, -0.02698064, -0.04436159,\n",
              "         0.01982918, -0.03604371, -0.04102348, -0.01662046,  0.00302916,\n",
              "         0.00992193,  0.02913162,  0.014605  ,  0.04935087, -0.03856333,\n",
              "         0.04460001,  0.01986302,  0.03709714, -0.00930188,  0.03951855,\n",
              "         0.02944975, -0.00949049,  0.00062913,  0.01739619, -0.0043362 ,\n",
              "        -0.02397143, -0.03394495,  0.02427847,  0.02030287,  0.02982292,\n",
              "        -0.00758438,  0.03660227, -0.01473581, -0.0185643 ,  0.02060458,\n",
              "        -0.0002015 , -0.02461776, -0.03663777], dtype=float32)>,\n",
              " TensorShape([128]),\n",
              " 'A')"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modelling a text dataset(running a series of experiments)\n",
        "\n",
        "* Model 0: Naive Bayes with Tokenization\n",
        "* Model 1: Feed-forward Neural Network(Dense Model)\n",
        "* Model 2: LSTM Model(RNN)\n",
        "* Model 3: GRU Model\n",
        "* Moel 4: Bidirectional LSTM\n",
        "* Model 5 : 1D Convolutional Layer\n",
        "* model 6: Tensorflow hub pretrained feature extraction (using transfer learning)\n",
        "* Model 7 : Same as model 6 with 10% of data\n",
        "\n",
        "How are we going to approach all of these:\n",
        "Use the standard steps in modelling with tensorflow\n",
        "\n",
        "* Create->build->fit->Evaluate\n",
        "\n"
      ],
      "metadata": {
        "id": "FhiG7RYnF6Le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 0\n",
        "\n",
        "To create a baseline, we'll use sklearn's Naive Bayes using Tf-IDF to convert our words to numbers\n",
        "\n",
        "It is a good practice to use non-DL Algos as a baseline because of their speed and then later use DL to see if you can improve upon them"
      ],
      "metadata": {
        "id": "aXUkwxQcI759"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "SgYwLWoxZ-rB"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0=Pipeline([\n",
        "    (\"tfidf\",TfidfVectorizer()),\n",
        "    (\"clf\",MultinomialNB())\n",
        "])\n",
        "\n",
        "model_0.fit(train_sentences, train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "c2zhqUaGZ_un",
        "outputId": "392ce67c-cf39-49e1-a927-2f5763b1da1f"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the baseline Model\n",
        "baseline_score = model_0.score(val_sentences, val_labels)\n",
        "print(f\"our baseline model achieves an accuracy of : {baseline_score*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqpo7Zw4cuEp",
        "outputId": "70ca6643-38c8-4f8e-cd7e-5b8204131625"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our baseline model achieves an accuracy of : 79.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.target.value_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK5L1LTydGik",
        "outputId": "1eb1846d-3676-4b57-f0d7-994e04f2910a"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method IndexOpsMixin.value_counts of 0       1\n",
              "1       1\n",
              "2       1\n",
              "3       1\n",
              "4       1\n",
              "       ..\n",
              "7608    1\n",
              "7609    1\n",
              "7610    1\n",
              "7611    1\n",
              "7612    1\n",
              "Name: target, Length: 7613, dtype: int64>"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Make predictions\n",
        "baseline_preds= model_0.predict(val_sentences)\n",
        "baseline_preds[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSf0XjLidIru",
        "outputId": "33c42a62-f890-48b4-a2dd-9389e2f431e3"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an evaluation function for our model experiments"
      ],
      "metadata": {
        "id": "f7C2BDI1dUx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could evaluate all of our model's predictions with different metrics every time.\n",
        "Calculate:\n",
        "* Accuracy\n",
        "* Precision\n",
        "* Recall\n",
        "* F1 score\n"
      ],
      "metadata": {
        "id": "fsfPdfPdeBYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to evaluate the above metrics\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculated model accuracy, precision, recall and f1 score of a binary classification model\n",
        "  \"\"\"\n",
        "  #Calculate model accuracy\n",
        "  model_accuracy=accuracy_score(y_true, y_pred) * 100\n",
        "  #Calculate model precision, recall and f1 score using the\"weighted average\"\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred,average=\"weighted\" )\n",
        "  model_results={'accuracy':model_accuracy,\n",
        "                 'precision': model_precision,\n",
        "                 'recall': model_recall,\n",
        "                 'f1-score':model_f1}\n",
        "  return model_results"
      ],
      "metadata": {
        "id": "w3KO2bNCeH9x"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get baseline results\n",
        "baseline_results=calculate_results(y_true=val_labels,\n",
        "                                   y_pred=baseline_preds)"
      ],
      "metadata": {
        "id": "ZRhBrZUceYKk"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results['accuracy']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5u_ypVpggRH",
        "outputId": "dab587d1-4cf4-4541-d929-16dc10fd1b6a"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79.26509186351706"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgSLKFUigjOp",
        "outputId": "ace667e9-789a-4ad5-a277-7e86d520ef7c"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1-score': 0.7862189758049549}"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: A simple Dense Model"
      ],
      "metadata": {
        "id": "GkA8wqsZgkFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a tensorflow callback\n",
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "#Create a directory to save tensorboard logs\n",
        "SAVE_DIR=\"model_logs\"\n"
      ],
      "metadata": {
        "id": "4UMvTAO0lRnx"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build model with functional API\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs=layers.Input(shape=(1,), dtype=tf.string) #Inputs are 1 dimensional\n",
        "x= text_vectorizer(inputs) #Turn the input text into numbers\n",
        "x= embedding(x) #Create an embedding of vctorized embeddings\n",
        "x=layers.GlobalAveragePooling1D()(x)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_1=tf.keras.Model(inputs, outputs, name=\"Model_1_dense\")"
      ],
      "metadata": {
        "id": "v-YXqNhHlXFp"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGntW8rbmRG_",
        "outputId": "91adcc2f-aea2-4658-ea93-ff3a570769fb"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Model_1_dense\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_16 (InputLayer)       [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_3 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " global_average_pooling1d_1   (None, 128)              0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,280,129\n",
            "Trainable params: 1,280,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile model\n",
        "model_1.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "AtH4W9MamYbc"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_history=model_1.fit(x=train_sentences,\n",
        "                            y=train_labels,\n",
        "                            epochs=5,\n",
        "                            validation_data=(val_sentences,val_labels),\n",
        "                            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
        "                                                                   experiment_name=\"model_1_dense\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG4M-QjhnJEQ",
        "outputId": "1be19ba3-47c7-4bf9-f747-9f7408c0d555"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/model_1_dense/20230616-031253\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 8s 31ms/step - loss: 0.6107 - accuracy: 0.6949 - val_loss: 0.5365 - val_accuracy: 0.7598\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 9s 42ms/step - loss: 0.4408 - accuracy: 0.8203 - val_loss: 0.4684 - val_accuracy: 0.7822\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 8s 36ms/step - loss: 0.3470 - accuracy: 0.8619 - val_loss: 0.4617 - val_accuracy: 0.7940\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 10s 47ms/step - loss: 0.2837 - accuracy: 0.8917 - val_loss: 0.4646 - val_accuracy: 0.7861\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 8s 39ms/step - loss: 0.2377 - accuracy: 0.9115 - val_loss: 0.4793 - val_accuracy: 0.7822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.evaluate(val_sentences, val_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWArT8vcnekL",
        "outputId": "3c92c11a-df40-4fea-a20f-6bfd5fa0754d"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4793 - accuracy: 0.7822\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4793274998664856, 0.7821522355079651]"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_pred_probs=model_1.predict(val_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QoASaI1nn5Y",
        "outputId": "079f05fc-dfb4-476e-9e5c-9dd381cc5b0d"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 1s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_pred_probs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYMwZRAMntde",
        "outputId": "2f0390f0-0a3a-46af-ee92-04d5034fcdc0"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(762, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate_results(val_labels, model_1_pred_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "x3zF16Kjon-Y",
        "outputId": "c993c456-a630-40be-a2bf-822e3c436b70"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-139-29ba58bcbd72>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_1_pred_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-127-bf803e99326d>\u001b[0m in \u001b[0;36mcalculate_results\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \"\"\"\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#Calculate model accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mmodel_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;31m#Calculate model precision, recall and f1 score using the\"weighted average\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mmodel_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weighted\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     97\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DefvW6HlpEhM",
        "outputId": "94aa980e-99fe-463a-95fe-0d89dac1957a"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
              "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
              "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
              "       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
              "       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
              "       1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
              "       1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
              "       0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
              "       1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
              "       1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
              "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Model_prediction Probabs into label formats\n",
        "model_1_preds=tf.squeeze(tf.round(model_1_pred_probs))"
      ],
      "metadata": {
        "id": "Siu3N9yupHrE"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5_rn_dtpMSZ",
        "outputId": "56133799-ead2-426f-9943-65d29a18d922"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(762,), dtype=float32, numpy=\n",
              "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "       0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
              "       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
              "       1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
              "       0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
              "       0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
              "       0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
              "       0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
              "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
              "       0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
              "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results=calculate_results(y_true=val_labels,\n",
        "                                  y_pred=model_1_preds)"
      ],
      "metadata": {
        "id": "e4LzOiRYqp97"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbsaMJeNq0-H",
        "outputId": "5829569e-4997-4f83-8afd-05e9aa98fdfd"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 78.21522309711287,\n",
              " 'precision': 0.7864332425219001,\n",
              " 'recall': 0.7821522309711286,\n",
              " 'f1-score': 0.7792361147360404}"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS3Y8aGrq2r3",
        "outputId": "5ffaa661-bc11-418b-c4d2-cdbb320c4267"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1-score': 0.7862189758049549}"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.array(list(model_1_results.values()))> np.array(list(baseline_results.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pSLUJ6Zq4a8",
        "outputId": "84496982-b2b9-4146-acf7-24aa12bedf33"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the learned embeddings\n"
      ],
      "metadata": {
        "id": "xYmbF71CrAkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the vocabulary from the text vectorization layer\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()"
      ],
      "metadata": {
        "id": "3jyHcUrWtyhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_in_vocab), words_in_vocab[:10]"
      ],
      "metadata": {
        "id": "DjOpoiLUt4T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1 summary\n",
        "model_1.summary()"
      ],
      "metadata": {
        "id": "3iZQmS3ouAsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the weight matrix of the embedding layers\n",
        "#These are numerical representation of our numerical datawhich are trained for 5 epochs\n",
        "\n",
        "embed_weights=model_1.get_layer(\"embedding\").get_weights()[0]\n",
        "embed_weights"
      ],
      "metadata": {
        "id": "cY5xpTiVuHbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embed_weights.shape)"
      ],
      "metadata": {
        "id": "_IDMc9ssuec6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got the embedding matrix our model has learned to represent tokens, let's see how we can visualize it\n",
        "\n",
        "To do so, tensorflow has a tool called Projector"
      ],
      "metadata": {
        "id": "lxSNxbJPusKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Code below is adapted from: https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\n",
        "import io\n",
        "\n",
        "# Create output writers\n",
        "out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
        "out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
        "\n",
        "# Write embedding vectors and words to file\n",
        "for num, word in enumerate(words_in_vocab):\n",
        "  if num == 0:\n",
        "     continue # skip padding token\n",
        "  vec = embed_weights[num]\n",
        "  out_m.write(word + \"\\n\") # write words to file\n",
        "  out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "metadata": {
        "id": "AWIx-3ehvLck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download files locally to upload to Embedding Projector\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download(\"embedding_vectors.tsv\")\n",
        "  files.download(\"embedding_metadata.tsv\")"
      ],
      "metadata": {
        "id": "5RPlhbYAwOBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks(RNNs)\n",
        "\n",
        "the premise of the Recurrent Neural Networs is to use the representation of the previous input to aid the representation of the later input\n",
        "\n",
        "RNNs are used for sequence data (this example is a sequence of text)"
      ],
      "metadata": {
        "id": "CrZT_hGHyOfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 📖 Resources:\n",
        "\n",
        "MIT Deep Learning Lecture on Recurrent Neural Networks - explains the background of recurrent neural networks and introduces LSTMs.\n",
        "The Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy - demonstrates the power of RNN's with examples generating various sequences.\n",
        "Understanding LSTMs by Chris Olah - an in-depth (and technical) look at the mechanics of the LSTM cell, possibly the most popular RNN building block."
      ],
      "metadata": {
        "id": "K_J8CWsiCbyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: LSTM\n",
        "\n",
        "LSTM= long short term memory\n",
        "\n",
        "Our structure of an RNN Looks like this\n",
        "\n",
        "Input(text)-> Tokenize -> Embedding -> Layers(RNNs/Dense)->output(label probability)"
      ],
      "metadata": {
        "id": "JIpS5KdMDdhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an LSTM Model\n",
        "from tensorflow.keras import layers\n",
        "inputs= layers.Input(shape=(1,), dtype=tf.string)\n",
        "x=text_vectorizer(inputs)\n",
        "x= embedding(x)\n",
        "print(x.shape)\n",
        "x=layers.LSTM(64, return_sequences=True)(x) #64 is hidden units\n",
        "#When you a stacking RNN Cells together, you need to return sequences=true\n",
        "print(x.shape)\n",
        "x= layers.LSTM(64)(x)\n",
        "print(x.shape)\n",
        "x= layers.Dense(64, activation='relu')(x)\n",
        "print(x.shape)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_2= tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")\n"
      ],
      "metadata": {
        "id": "hA922aKJASoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an LSTM Model\n",
        "from tensorflow.keras import layers\n",
        "inputs= layers.Input(shape=(1,), dtype=tf.string)\n",
        "x=text_vectorizer(inputs)\n",
        "x= embedding(x)\n",
        "#print(x.shape)\n",
        "#x=layers.LSTM(64, return_sequences=True)(x) #64 is hidden units\n",
        "#When you a stacking RNN Cells together, you need to return sequences=true\n",
        "#print(x.shape)\n",
        "x= layers.LSTM(64)(x)\n",
        "#print(x.shape)\n",
        "#x= layers.Dense(64, activation='relu')(x)\n",
        "#print(x.shape)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_2= tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")\n"
      ],
      "metadata": {
        "id": "7nHB-E4PBuQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "id": "FzlIZIY8Dm7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The default activation function for RNNs is TanH"
      ],
      "metadata": {
        "id": "QxsN2GykDo_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Complie the model\n",
        "model_2.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "rf9WWNf9D1xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_model_2_LSTM=model_2.fit(train_sentences,\n",
        "                                 train_labels,\n",
        "                                 epochs=5,\n",
        "                                 validation_data=(val_sentences, val_labels),\n",
        "                                 callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
        "                                                                        experiment_name=\"model_2_LSTM\")])"
      ],
      "metadata": {
        "id": "qfH-KOfKEI_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_pred_probs=model_2.predict(val_sentences)\n",
        "model_2_pred_probs[:10]"
      ],
      "metadata": {
        "id": "n5qVJNf2EY7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the probabs to pred\n",
        "model_2_preds=tf.squeeze(tf.round(model_2_pred_probs))"
      ],
      "metadata": {
        "id": "ZSymwXH5EjXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_preds\n"
      ],
      "metadata": {
        "id": "xfA-YNydEuin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_results=calculate_results(y_true=val_labels,\n",
        "                                  y_pred=model_2_preds)"
      ],
      "metadata": {
        "id": "F4Nt6h5PEwPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_results"
      ],
      "metadata": {
        "id": "kRUgB8tnE2Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results\n"
      ],
      "metadata": {
        "id": "_y-bd5d6E3Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: GRU\n",
        "\n",
        "Similar to LSTM but has less params"
      ],
      "metadata": {
        "id": "UXF740ofE4nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "inputs= tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "\n",
        "x=embedding(x)\n",
        "\n",
        "x = tf.keras.layers.GRU(64, return_sequences=True)(x) #if you want to stack recurrent layers on top of each other, you need to use return_sequences=True\n",
        "x=layers.LSTM(64, return_sequences=True)(x)\n",
        "x=layers.GRU(64)(x)\n",
        "\n",
        "x=layers.Dense(64, activation='relu')(x)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(x)\n",
        "model_3=tf.keras.Model(inputs, outputs, name=\"Model_3_GRU\")\n"
      ],
      "metadata": {
        "id": "co1zgceRGAbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "inputs= tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "\n",
        "x=embedding(x)\n",
        "\n",
        "x = tf.keras.layers.GRU(64)(x) #if you want to stack recurrent layers on top of each other, you need to use return_sequences=True\n",
        "#x=layers.LSTM(64, return_sequences=True)(x)\n",
        "#x=layers.GRU(64)(x)\n",
        "\n",
        "x=layers.Dense(64, activation='relu')(x)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(x)\n",
        "model_3=tf.keras.Model(inputs, outputs, name=\"Model_3_GRU\")\n"
      ],
      "metadata": {
        "id": "DY1pSYYhIGWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.summary()"
      ],
      "metadata": {
        "id": "4KKTktRsHIl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "j-bjh1F5HkL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_model_3_GRU=model_3.fit(train_sentences,\n",
        "                                train_labels,\n",
        "                                validation_data=(val_sentences, val_labels),\n",
        "                                epochs=5,\n",
        "                                callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
        "                                                                      experiment_name='model_name_GRU')])"
      ],
      "metadata": {
        "id": "xCnE4GUYIBBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_pred_probs=model_3.predict(val_sentences)\n",
        "model_3_pred_probs[:10]"
      ],
      "metadata": {
        "id": "2bOgGL-qJHvD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79caed18-f959-4eee-a231-be0710f97715"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 1s 8ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.2693977e-03],\n",
              "       [5.5768818e-01],\n",
              "       [9.9999583e-01],\n",
              "       [1.0364279e-01],\n",
              "       [1.1872800e-05],\n",
              "       [9.9975604e-01],\n",
              "       [8.8811561e-02],\n",
              "       [9.9999785e-01],\n",
              "       [9.9999595e-01],\n",
              "       [6.8013978e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_preds=tf.squeeze(tf.round(model_3_pred_probs))\n",
        "model_3_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_pNU9DZMcnG",
        "outputId": "79a82ce9-00b6-42ea-c666-28c626658501"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 0., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_results(y_true=val_labels,\n",
        "                  y_pred=model_3_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYGuZrfxMnza",
        "outputId": "437e5e79-c001-449d-8fe3-505cab44fcb6"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 76.24671916010499,\n",
              " 'precision': 0.7639905060800456,\n",
              " 'recall': 0.7624671916010499,\n",
              " 'f1-score': 0.7602456873168202}"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYltXXH5MuZe",
        "outputId": "13618939-b5c3-4d97-f548-8d297c836b7f"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1-score': 0.7862189758049549}"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model_4 : Bidirectional LSTM\n",
        "\n",
        "Normal RNNs go from left to right..\n",
        "Bidirectional RNNs go from left to right as well as right to left\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "45Q_Fz5nMwZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "inputs=layers.Input(shape=(1,), dtype=tf.string)\n",
        "x= text_vectorizer(inputs)\n",
        "\n",
        "x= embedding(x)\n",
        "\n",
        "x=layers.Bidirectional(layers.LSTM(64))(x)\n",
        "#print(x.shape) #Doubles the value of the shape...as the cell is bidirectional\n",
        "#x=layers.Bidirectional(layers.GRU(64))(x)\n",
        "\n",
        "outputs=layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_4=tf.keras.Model(inputs, outputs, name='model_4_bidirectional')"
      ],
      "metadata": {
        "id": "yExqxgfRP-sq"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.summary()"
      ],
      "metadata": {
        "id": "df9ldRCqR3to",
        "outputId": "7cd2235d-fb77-453a-ed49-58229a8e05a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4_bidirectional\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_22 (InputLayer)       [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_3 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,378,945\n",
            "Trainable params: 1,378,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.compile(loss='binary_crossentropy',\n",
        "                metrics=['accuracy'],\n",
        "                optimizer=tf.keras.optimizers.Adam())"
      ],
      "metadata": {
        "id": "eo_029tXSGlh"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_4_bidirectional=model_4.fit(train_sentences,\n",
        "                                    train_labels,\n",
        "                                    validation_data=(val_sentences, val_labels),\n",
        "                                    epochs=5,\n",
        "                                    callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
        "                                                                           experiment_name='model_4_bidirectional')])"
      ],
      "metadata": {
        "id": "tv1JVDxMS-Bd",
        "outputId": "48e6bbe0-be62-48a4-f06e-ee7276f67f98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/model_4_bidirectional/20230616-034334\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 17s 54ms/step - loss: 0.2152 - accuracy: 0.9225 - val_loss: 0.5480 - val_accuracy: 0.7835\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 11s 49ms/step - loss: 0.1549 - accuracy: 0.9429 - val_loss: 0.6283 - val_accuracy: 0.7822\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 10s 48ms/step - loss: 0.1271 - accuracy: 0.9514 - val_loss: 0.7197 - val_accuracy: 0.7743\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 9s 43ms/step - loss: 0.1084 - accuracy: 0.9574 - val_loss: 0.7231 - val_accuracy: 0.7769\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 14s 64ms/step - loss: 0.0892 - accuracy: 0.9654 - val_loss: 0.9359 - val_accuracy: 0.7677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_4_pred_probs=model_4.predict(val_sentences)\n",
        "model_4_pred_probs[:10]"
      ],
      "metadata": {
        "id": "bnbBSNVWTRI_",
        "outputId": "985973ca-488c-4322-f51b-d8dbcecf2cb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 4s 8ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.1251854e-01],\n",
              "       [9.0274268e-01],\n",
              "       [9.9991882e-01],\n",
              "       [5.6247890e-02],\n",
              "       [1.3182372e-04],\n",
              "       [9.9746835e-01],\n",
              "       [9.1958749e-01],\n",
              "       [9.9996579e-01],\n",
              "       [9.9990803e-01],\n",
              "       [5.2167481e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_4_preds=tf.squeeze(tf.round(model_4_pred_probs))\n",
        "model_4_preds"
      ],
      "metadata": {
        "id": "8W5brRHbTmU1",
        "outputId": "1189fa55-025a-44b5-a8ae-584897e94eb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(762,), dtype=float32, numpy=\n",
              "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
              "       0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
              "       1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
              "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
              "       0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
              "       0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
              "       0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
              "       0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
              "       0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
              "       0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
              "       0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
              "       0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_4_results=calculate_results(y_true=val_labels,\n",
        "                y_pred=model_4_preds)"
      ],
      "metadata": {
        "id": "05u_VmVITt6m"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4_results"
      ],
      "metadata": {
        "id": "ja-R9v37T7eS",
        "outputId": "d3f8c474-e6c6-4aa5-b098-3ad01b03cd87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 76.77165354330708,\n",
              " 'precision': 0.7681410880728078,\n",
              " 'recall': 0.7677165354330708,\n",
              " 'f1-score': 0.7662770891654436}"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "id": "NgK46Tz_Tx7P",
        "outputId": "4f72f8b9-a1ac-449a-af76-20bbbdd79926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1-score': 0.7862189758049549}"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KioDZiajT8gS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}